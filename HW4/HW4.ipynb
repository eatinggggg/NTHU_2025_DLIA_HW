{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**"
      ],
      "metadata": {
        "id": "bKKUU729mk5L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceRyCMLZB9Hl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkFNeQMNB9bX",
        "outputId": "2fb75512-2c8d-424d-c300-7c34c7294ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/DL_HW4/nvda.us.txt')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Nhdz5pPOCB13",
        "outputId": "d6fa7cac-7ad2-4667-aba2-216fe4aee205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Date      Open      High       Low     Close    Volume  OpenInt\n",
              "0     1999-01-22    1.6238    1.8092    1.4379    1.5215  18297633        0\n",
              "1     1999-01-25    1.6423    1.6980    1.5215    1.6793   3445279        0\n",
              "2     1999-01-26    1.6980    1.7350    1.5311    1.5494   2320993        0\n",
              "3     1999-01-27    1.5586    1.5957    1.4660    1.5494   1662658        0\n",
              "4     1999-01-28    1.5494    1.5586    1.5311    1.5401   1540110        0\n",
              "...          ...       ...       ...       ...       ...       ...      ...\n",
              "4728  2017-11-06  207.2000  209.9800  206.7000  209.6300   9731783        0\n",
              "4729  2017-11-07  210.5500  212.9000  210.0560  212.0000  10671815        0\n",
              "4730  2017-11-08  211.8500  212.0000  207.2400  209.1600  13033902        0\n",
              "4731  2017-11-09  205.2700  206.3300  200.3700  205.3200  23895006        0\n",
              "4732  2017-11-10  213.0800  218.6700  211.6300  216.1400  31300857        0\n",
              "\n",
              "[4733 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e1aca31-0c97-4f53-b689-7d4595710c53\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>OpenInt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1999-01-22</td>\n",
              "      <td>1.6238</td>\n",
              "      <td>1.8092</td>\n",
              "      <td>1.4379</td>\n",
              "      <td>1.5215</td>\n",
              "      <td>18297633</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1999-01-25</td>\n",
              "      <td>1.6423</td>\n",
              "      <td>1.6980</td>\n",
              "      <td>1.5215</td>\n",
              "      <td>1.6793</td>\n",
              "      <td>3445279</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1999-01-26</td>\n",
              "      <td>1.6980</td>\n",
              "      <td>1.7350</td>\n",
              "      <td>1.5311</td>\n",
              "      <td>1.5494</td>\n",
              "      <td>2320993</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1999-01-27</td>\n",
              "      <td>1.5586</td>\n",
              "      <td>1.5957</td>\n",
              "      <td>1.4660</td>\n",
              "      <td>1.5494</td>\n",
              "      <td>1662658</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1999-01-28</td>\n",
              "      <td>1.5494</td>\n",
              "      <td>1.5586</td>\n",
              "      <td>1.5311</td>\n",
              "      <td>1.5401</td>\n",
              "      <td>1540110</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4728</th>\n",
              "      <td>2017-11-06</td>\n",
              "      <td>207.2000</td>\n",
              "      <td>209.9800</td>\n",
              "      <td>206.7000</td>\n",
              "      <td>209.6300</td>\n",
              "      <td>9731783</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4729</th>\n",
              "      <td>2017-11-07</td>\n",
              "      <td>210.5500</td>\n",
              "      <td>212.9000</td>\n",
              "      <td>210.0560</td>\n",
              "      <td>212.0000</td>\n",
              "      <td>10671815</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4730</th>\n",
              "      <td>2017-11-08</td>\n",
              "      <td>211.8500</td>\n",
              "      <td>212.0000</td>\n",
              "      <td>207.2400</td>\n",
              "      <td>209.1600</td>\n",
              "      <td>13033902</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4731</th>\n",
              "      <td>2017-11-09</td>\n",
              "      <td>205.2700</td>\n",
              "      <td>206.3300</td>\n",
              "      <td>200.3700</td>\n",
              "      <td>205.3200</td>\n",
              "      <td>23895006</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4732</th>\n",
              "      <td>2017-11-10</td>\n",
              "      <td>213.0800</td>\n",
              "      <td>218.6700</td>\n",
              "      <td>211.6300</td>\n",
              "      <td>216.1400</td>\n",
              "      <td>31300857</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4733 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e1aca31-0c97-4f53-b689-7d4595710c53')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2e1aca31-0c97-4f53-b689-7d4595710c53 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2e1aca31-0c97-4f53-b689-7d4595710c53');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2ce08f69-7875-49df-b0a8-b239bd1604b7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2ce08f69-7875-49df-b0a8-b239bd1604b7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2ce08f69-7875-49df-b0a8-b239bd1604b7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_6483f9c3-e422-4583-8d2b-df0158c392c8\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6483f9c3-e422-4583-8d2b-df0158c392c8 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4733,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 4733,\n        \"samples\": [\n          \"2008-03-28\",\n          \"2000-06-19\",\n          \"2004-08-16\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Open\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30.236283591120817,\n        \"min\": 1.2989,\n        \"max\": 213.08,\n        \"num_unique_values\": 2831,\n        \"samples\": [\n          18.532,\n          34.411,\n          21.283\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30.615622405876827,\n        \"min\": 1.3175,\n        \"max\": 218.67,\n        \"num_unique_values\": 2873,\n        \"samples\": [\n          8.313,\n          18.421,\n          32.565\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29.792957853879482,\n        \"min\": 1.2339,\n        \"max\": 211.63,\n        \"num_unique_values\": 2820,\n        \"samples\": [\n          21.683,\n          27.176,\n          6.5133\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30.263704772360708,\n        \"min\": 1.2619,\n        \"max\": 216.14,\n        \"num_unique_values\": 2885,\n        \"samples\": [\n          14.158,\n          20.894,\n          25.967\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12724458,\n        \"min\": 0,\n        \"max\": 249665903,\n        \"num_unique_values\": 4720,\n        \"samples\": [\n          10694895,\n          25158762,\n          4576347\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"OpenInt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot = df.plot('Date', 'High', figsize=(15, 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "EgdhETViCB34",
        "outputId": "ebf957a0-1dd4-44a2-8666-70d619fe9e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG31uW9WCB54",
        "outputId": "bccbcdbd-5d98-4ec9-976a-5964a8486405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'OpenInt'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9KmedWICB7n",
        "outputId": "b69b26d6-4ac2-47bf-d8c0-9d47e467c062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4733 entries, 0 to 4732\n",
            "Data columns (total 7 columns):\n",
            " #   Column   Non-Null Count  Dtype  \n",
            "---  ------   --------------  -----  \n",
            " 0   Date     4733 non-null   object \n",
            " 1   Open     4733 non-null   float64\n",
            " 2   High     4733 non-null   float64\n",
            " 3   Low      4733 non-null   float64\n",
            " 4   Close    4733 non-null   float64\n",
            " 5   Volume   4733 non-null   int64  \n",
            " 6   OpenInt  4733 non-null   int64  \n",
            "dtypes: float64(4), int64(2), object(1)\n",
            "memory usage: 259.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for null values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "W0JvzzQWCWZr",
        "outputId": "4b49ae0f-3f2a-4ebd-e11e-2612ab97a203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date       0\n",
              "Open       0\n",
              "High       0\n",
              "Low        0\n",
              "Close      0\n",
              "Volume     0\n",
              "OpenInt    0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Open</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>High</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Low</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Close</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Volume</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OpenInt</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "n_x1uMxdCWbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhGrhsAxCWd3",
        "outputId": "a16a917e-8880-4c03-d52d-10c29014bd19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4733, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3 combinations of window size & step**"
      ],
      "metadata": {
        "id": "GNux2v7jnBcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "def run_experiment(df, window_size, step, with_volume=False, seed=42):\n",
        "    # 設定隨機種子\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # 特徵選擇\n",
        "    if with_volume:\n",
        "        features = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "        input_dim = 5\n",
        "    else:\n",
        "        features = df[['Open', 'High', 'Low', 'Close']]\n",
        "        input_dim = 4\n",
        "    labels = df['High'].shift(-1).dropna()\n",
        "    features = features.iloc[:-1]\n",
        "\n",
        "    # 建立序列\n",
        "    def create_sequences(input_data, output_data, window_size, step):\n",
        "        sequences, labels = [], []\n",
        "        for i in range(0, len(input_data) - window_size, step):\n",
        "            sequences.append(input_data[i:(i + window_size)])\n",
        "            labels.append(output_data[i + window_size])\n",
        "        return np.array(sequences), np.array(labels)\n",
        "\n",
        "    X, y = create_sequences(features.values, labels.values, window_size, step)\n",
        "\n",
        "    # 測試集切分（10%）\n",
        "    ind = np.linspace(0, len(X) - 1, num=int(len(X) * 0.1), dtype=int)\n",
        "    x_test_raw, y_test = X[ind], y[ind]\n",
        "    X, y = np.delete(X, ind, axis=0), np.delete(y, ind, axis=0)\n",
        "\n",
        "    # 資料切分\n",
        "    idx = np.random.permutation(len(X))\n",
        "    X, y = X[idx], y[idx]\n",
        "    split = int(len(X) * 0.8)\n",
        "    x_train_raw, y_train = X[:split], y[:split]\n",
        "    x_val_raw, y_val = X[split:], y[split:]\n",
        "\n",
        "    # 轉換為 Tensor（不做 normalization）\n",
        "    x_train = torch.from_numpy(x_train_raw).float()\n",
        "    y_train = torch.from_numpy(y_train).float()\n",
        "    x_val = torch.from_numpy(x_val_raw).float()\n",
        "    y_val = torch.from_numpy(y_val).float()\n",
        "    x_test = torch.from_numpy(x_test_raw).float()\n",
        "    y_test = torch.from_numpy(y_test).float()\n",
        "\n",
        "    # 資料載入器\n",
        "    train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(x_val, y_val), batch_size=32)\n",
        "    test_loader = DataLoader(TensorDataset(x_test, y_test), batch_size=32)\n",
        "\n",
        "    # LSTM 模型\n",
        "    class LSTMModel(nn.Module):\n",
        "        def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "            super().__init__()\n",
        "            self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        def forward(self, x):\n",
        "            out, _ = self.lstm(x)\n",
        "            return self.fc(out[:, -1, :])\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = LSTMModel(input_dim=input_dim, hidden_dim=500, num_layers=1, output_dim=1).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        total_loss=0\n",
        "        for features, labels in train_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            outputs = model(features).squeeze(-1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for features, labels in val_loader:\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "                outputs = model(features).squeeze(-1)\n",
        "                val_loss += criterion(outputs, labels).item()\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f'Epoch {epoch+1}/100, Train loss: {avg_train_loss:.4f}, Val loss: {avg_val_loss:.4f}, Best Val loss: {best_val_loss:.4f}')\n",
        "\n",
        "    # 測試\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    model.eval()\n",
        "    preds, actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for features, labels in test_loader:\n",
        "            features = features.to(device)\n",
        "            pred = model(features).squeeze(-1).cpu()\n",
        "            preds.append(pred)\n",
        "            actuals.append(labels)\n",
        "    pred_value = torch.cat(preds).numpy()\n",
        "    actual_value = torch.cat(actuals).numpy()\n",
        "\n",
        "    # MSE 計算\n",
        "    test_mse = np.mean((pred_value - actual_value) ** 2)\n",
        "    return round(test_mse, 6)\n"
      ],
      "metadata": {
        "id": "kWMdQdBfoK8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = [\n",
        "    (10, 5),   # window > step\n",
        "    (10, 10),  # window = step\n",
        "    (5, 10)    # window < step\n",
        "]\n",
        "results = []\n",
        "\n",
        "for w, s in params:\n",
        "    mse = run_experiment(df, window_size=w, step=s, with_volume=False)\n",
        "    results.append({'Window Size': w, 'Step Size': s, 'MSE (Raw Input)': mse})\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WZNIdD7zoLAm",
        "outputId": "cf17ed3f-fa52-4642-b2fe-d9407c0f3f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train loss: 1150.2249, Val loss: 693.2274, Best Val loss: 693.2274\n",
            "Epoch 2/100, Train loss: 879.0414, Val loss: 570.8699, Best Val loss: 570.8699\n",
            "Epoch 3/100, Train loss: 908.0383, Val loss: 494.7178, Best Val loss: 494.7178\n",
            "Epoch 4/100, Train loss: 748.1249, Val loss: 429.6195, Best Val loss: 429.6195\n",
            "Epoch 5/100, Train loss: 658.2494, Val loss: 378.7449, Best Val loss: 378.7449\n",
            "Epoch 6/100, Train loss: 493.6470, Val loss: 332.2879, Best Val loss: 332.2879\n",
            "Epoch 7/100, Train loss: 479.4180, Val loss: 297.6078, Best Val loss: 297.6078\n",
            "Epoch 8/100, Train loss: 424.8443, Val loss: 265.7236, Best Val loss: 265.7236\n",
            "Epoch 9/100, Train loss: 339.5275, Val loss: 237.7978, Best Val loss: 237.7978\n",
            "Epoch 10/100, Train loss: 299.0137, Val loss: 212.2130, Best Val loss: 212.2130\n",
            "Epoch 11/100, Train loss: 273.6637, Val loss: 192.3648, Best Val loss: 192.3648\n",
            "Epoch 12/100, Train loss: 240.1963, Val loss: 170.6134, Best Val loss: 170.6134\n",
            "Epoch 13/100, Train loss: 212.0147, Val loss: 155.9253, Best Val loss: 155.9253\n",
            "Epoch 14/100, Train loss: 194.1329, Val loss: 142.6819, Best Val loss: 142.6819\n",
            "Epoch 15/100, Train loss: 216.1028, Val loss: 130.9154, Best Val loss: 130.9154\n",
            "Epoch 16/100, Train loss: 157.2179, Val loss: 115.5566, Best Val loss: 115.5566\n",
            "Epoch 17/100, Train loss: 138.8165, Val loss: 104.9323, Best Val loss: 104.9323\n",
            "Epoch 18/100, Train loss: 125.4453, Val loss: 97.0439, Best Val loss: 97.0439\n",
            "Epoch 19/100, Train loss: 119.1166, Val loss: 91.5296, Best Val loss: 91.5296\n",
            "Epoch 20/100, Train loss: 108.5100, Val loss: 85.2916, Best Val loss: 85.2916\n",
            "Epoch 21/100, Train loss: 102.3062, Val loss: 78.1365, Best Val loss: 78.1365\n",
            "Epoch 22/100, Train loss: 93.1095, Val loss: 71.7182, Best Val loss: 71.7182\n",
            "Epoch 23/100, Train loss: 86.7472, Val loss: 67.3937, Best Val loss: 67.3937\n",
            "Epoch 24/100, Train loss: 79.2718, Val loss: 62.4023, Best Val loss: 62.4023\n",
            "Epoch 25/100, Train loss: 72.0194, Val loss: 58.3156, Best Val loss: 58.3156\n",
            "Epoch 26/100, Train loss: 66.3803, Val loss: 53.4724, Best Val loss: 53.4724\n",
            "Epoch 27/100, Train loss: 60.8784, Val loss: 49.3509, Best Val loss: 49.3509\n",
            "Epoch 28/100, Train loss: 55.2843, Val loss: 47.4764, Best Val loss: 47.4764\n",
            "Epoch 29/100, Train loss: 52.2616, Val loss: 43.2248, Best Val loss: 43.2248\n",
            "Epoch 30/100, Train loss: 48.7967, Val loss: 40.4459, Best Val loss: 40.4459\n",
            "Epoch 31/100, Train loss: 44.5994, Val loss: 37.2480, Best Val loss: 37.2480\n",
            "Epoch 32/100, Train loss: 40.3900, Val loss: 35.9441, Best Val loss: 35.9441\n",
            "Epoch 33/100, Train loss: 39.7185, Val loss: 33.9478, Best Val loss: 33.9478\n",
            "Epoch 34/100, Train loss: 35.7379, Val loss: 32.0330, Best Val loss: 32.0330\n",
            "Epoch 35/100, Train loss: 34.6602, Val loss: 30.3229, Best Val loss: 30.3229\n",
            "Epoch 36/100, Train loss: 31.9520, Val loss: 29.9789, Best Val loss: 29.9789\n",
            "Epoch 37/100, Train loss: 29.4470, Val loss: 26.7012, Best Val loss: 26.7012\n",
            "Epoch 38/100, Train loss: 26.7816, Val loss: 25.6940, Best Val loss: 25.6940\n",
            "Epoch 39/100, Train loss: 25.7951, Val loss: 24.7853, Best Val loss: 24.7853\n",
            "Epoch 40/100, Train loss: 23.5088, Val loss: 23.1491, Best Val loss: 23.1491\n",
            "Epoch 41/100, Train loss: 21.7675, Val loss: 22.1426, Best Val loss: 22.1426\n",
            "Epoch 42/100, Train loss: 29.2034, Val loss: 20.7781, Best Val loss: 20.7781\n",
            "Epoch 43/100, Train loss: 19.3597, Val loss: 19.3027, Best Val loss: 19.3027\n",
            "Epoch 44/100, Train loss: 17.9390, Val loss: 18.4904, Best Val loss: 18.4904\n",
            "Epoch 45/100, Train loss: 17.2521, Val loss: 17.5685, Best Val loss: 17.5685\n",
            "Epoch 46/100, Train loss: 15.7984, Val loss: 16.9981, Best Val loss: 16.9981\n",
            "Epoch 47/100, Train loss: 15.0770, Val loss: 16.3991, Best Val loss: 16.3991\n",
            "Epoch 48/100, Train loss: 14.1194, Val loss: 16.1225, Best Val loss: 16.1225\n",
            "Epoch 49/100, Train loss: 13.5526, Val loss: 15.2470, Best Val loss: 15.2470\n",
            "Epoch 50/100, Train loss: 12.9233, Val loss: 14.7099, Best Val loss: 14.7099\n",
            "Epoch 51/100, Train loss: 13.6546, Val loss: 15.2154, Best Val loss: 14.7099\n",
            "Epoch 52/100, Train loss: 12.3064, Val loss: 13.8845, Best Val loss: 13.8845\n",
            "Epoch 53/100, Train loss: 11.4525, Val loss: 13.9751, Best Val loss: 13.8845\n",
            "Epoch 54/100, Train loss: 11.2758, Val loss: 14.1713, Best Val loss: 13.8845\n",
            "Epoch 55/100, Train loss: 10.6461, Val loss: 12.9566, Best Val loss: 12.9566\n",
            "Epoch 56/100, Train loss: 10.3215, Val loss: 13.8564, Best Val loss: 12.9566\n",
            "Epoch 57/100, Train loss: 9.8463, Val loss: 12.0467, Best Val loss: 12.0467\n",
            "Epoch 58/100, Train loss: 10.0139, Val loss: 13.2621, Best Val loss: 12.0467\n",
            "Epoch 59/100, Train loss: 9.4133, Val loss: 12.4108, Best Val loss: 12.0467\n",
            "Epoch 60/100, Train loss: 9.2747, Val loss: 11.8547, Best Val loss: 11.8547\n",
            "Epoch 61/100, Train loss: 8.7745, Val loss: 11.7558, Best Val loss: 11.7558\n",
            "Epoch 62/100, Train loss: 8.6006, Val loss: 11.1830, Best Val loss: 11.1830\n",
            "Epoch 63/100, Train loss: 8.3245, Val loss: 11.7187, Best Val loss: 11.1830\n",
            "Epoch 64/100, Train loss: 8.2342, Val loss: 10.8755, Best Val loss: 10.8755\n",
            "Epoch 65/100, Train loss: 8.0099, Val loss: 11.1734, Best Val loss: 10.8755\n",
            "Epoch 66/100, Train loss: 7.8405, Val loss: 10.8844, Best Val loss: 10.8755\n",
            "Epoch 67/100, Train loss: 7.6544, Val loss: 10.8311, Best Val loss: 10.8311\n",
            "Epoch 68/100, Train loss: 7.4580, Val loss: 10.2302, Best Val loss: 10.2302\n",
            "Epoch 69/100, Train loss: 7.9941, Val loss: 10.7010, Best Val loss: 10.2302\n",
            "Epoch 70/100, Train loss: 7.4335, Val loss: 10.4993, Best Val loss: 10.2302\n",
            "Epoch 71/100, Train loss: 7.1444, Val loss: 10.4031, Best Val loss: 10.2302\n",
            "Epoch 72/100, Train loss: 7.0518, Val loss: 10.5293, Best Val loss: 10.2302\n",
            "Epoch 73/100, Train loss: 6.9428, Val loss: 9.9477, Best Val loss: 9.9477\n",
            "Epoch 74/100, Train loss: 6.9610, Val loss: 10.2211, Best Val loss: 9.9477\n",
            "Epoch 75/100, Train loss: 6.7963, Val loss: 9.8694, Best Val loss: 9.8694\n",
            "Epoch 76/100, Train loss: 6.7642, Val loss: 9.7368, Best Val loss: 9.7368\n",
            "Epoch 77/100, Train loss: 6.6367, Val loss: 9.8806, Best Val loss: 9.7368\n",
            "Epoch 78/100, Train loss: 6.5086, Val loss: 9.7853, Best Val loss: 9.7368\n",
            "Epoch 79/100, Train loss: 6.4183, Val loss: 9.6184, Best Val loss: 9.6184\n",
            "Epoch 80/100, Train loss: 6.6129, Val loss: 9.6213, Best Val loss: 9.6184\n",
            "Epoch 81/100, Train loss: 6.3232, Val loss: 9.6626, Best Val loss: 9.6184\n",
            "Epoch 82/100, Train loss: 6.6867, Val loss: 9.4810, Best Val loss: 9.4810\n",
            "Epoch 83/100, Train loss: 6.2706, Val loss: 9.6209, Best Val loss: 9.4810\n",
            "Epoch 84/100, Train loss: 6.1814, Val loss: 9.3988, Best Val loss: 9.3988\n",
            "Epoch 85/100, Train loss: 6.1482, Val loss: 9.3965, Best Val loss: 9.3965\n",
            "Epoch 86/100, Train loss: 6.3780, Val loss: 9.4242, Best Val loss: 9.3965\n",
            "Epoch 87/100, Train loss: 6.1661, Val loss: 9.4247, Best Val loss: 9.3965\n",
            "Epoch 88/100, Train loss: 6.2334, Val loss: 9.3422, Best Val loss: 9.3422\n",
            "Epoch 89/100, Train loss: 9.6431, Val loss: 9.2885, Best Val loss: 9.2885\n",
            "Epoch 90/100, Train loss: 6.0217, Val loss: 9.2974, Best Val loss: 9.2885\n",
            "Epoch 91/100, Train loss: 5.9982, Val loss: 9.3139, Best Val loss: 9.2885\n",
            "Epoch 92/100, Train loss: 6.0089, Val loss: 9.2918, Best Val loss: 9.2885\n",
            "Epoch 93/100, Train loss: 5.9817, Val loss: 9.3044, Best Val loss: 9.2885\n",
            "Epoch 94/100, Train loss: 5.9585, Val loss: 9.2885, Best Val loss: 9.2885\n",
            "Epoch 95/100, Train loss: 5.9537, Val loss: 9.2764, Best Val loss: 9.2764\n",
            "Epoch 96/100, Train loss: 6.2793, Val loss: 9.2744, Best Val loss: 9.2744\n",
            "Epoch 97/100, Train loss: 5.9425, Val loss: 9.2616, Best Val loss: 9.2616\n",
            "Epoch 98/100, Train loss: 5.9833, Val loss: 9.2582, Best Val loss: 9.2582\n",
            "Epoch 99/100, Train loss: 5.9430, Val loss: 9.2566, Best Val loss: 9.2566\n",
            "Epoch 100/100, Train loss: 5.9437, Val loss: 9.2565, Best Val loss: 9.2565\n",
            "Epoch 1/100, Train loss: 1051.0033, Val loss: 1598.3004, Best Val loss: 1598.3004\n",
            "Epoch 2/100, Train loss: 839.8434, Val loss: 1442.7604, Best Val loss: 1442.7604\n",
            "Epoch 3/100, Train loss: 768.8988, Val loss: 1345.9568, Best Val loss: 1345.9568\n",
            "Epoch 4/100, Train loss: 686.5020, Val loss: 1260.8802, Best Val loss: 1260.8802\n",
            "Epoch 5/100, Train loss: 653.7782, Val loss: 1187.2432, Best Val loss: 1187.2432\n",
            "Epoch 6/100, Train loss: 582.8137, Val loss: 1115.4014, Best Val loss: 1115.4014\n",
            "Epoch 7/100, Train loss: 556.4319, Val loss: 1059.3088, Best Val loss: 1059.3088\n",
            "Epoch 8/100, Train loss: 518.9940, Val loss: 1004.2064, Best Val loss: 1004.2064\n",
            "Epoch 9/100, Train loss: 454.5529, Val loss: 951.5014, Best Val loss: 951.5014\n",
            "Epoch 10/100, Train loss: 415.0455, Val loss: 903.6585, Best Val loss: 903.6585\n",
            "Epoch 11/100, Train loss: 392.9579, Val loss: 859.8388, Best Val loss: 859.8388\n",
            "Epoch 12/100, Train loss: 394.9014, Val loss: 819.7331, Best Val loss: 819.7331\n",
            "Epoch 13/100, Train loss: 352.7742, Val loss: 779.5264, Best Val loss: 779.5264\n",
            "Epoch 14/100, Train loss: 338.5883, Val loss: 740.9177, Best Val loss: 740.9177\n",
            "Epoch 15/100, Train loss: 328.7147, Val loss: 704.7033, Best Val loss: 704.7033\n",
            "Epoch 16/100, Train loss: 279.8355, Val loss: 669.8350, Best Val loss: 669.8350\n",
            "Epoch 17/100, Train loss: 262.3856, Val loss: 638.8301, Best Val loss: 638.8301\n",
            "Epoch 18/100, Train loss: 267.5117, Val loss: 611.2522, Best Val loss: 611.2522\n",
            "Epoch 19/100, Train loss: 254.1181, Val loss: 584.3170, Best Val loss: 584.3170\n",
            "Epoch 20/100, Train loss: 219.2987, Val loss: 554.5326, Best Val loss: 554.5326\n",
            "Epoch 21/100, Train loss: 205.8041, Val loss: 535.4901, Best Val loss: 535.4901\n",
            "Epoch 22/100, Train loss: 195.3930, Val loss: 506.2739, Best Val loss: 506.2739\n",
            "Epoch 23/100, Train loss: 196.6182, Val loss: 486.2014, Best Val loss: 486.2014\n",
            "Epoch 24/100, Train loss: 174.7537, Val loss: 466.1703, Best Val loss: 466.1703\n",
            "Epoch 25/100, Train loss: 165.8041, Val loss: 451.1546, Best Val loss: 451.1546\n",
            "Epoch 26/100, Train loss: 157.9028, Val loss: 430.1474, Best Val loss: 430.1474\n",
            "Epoch 27/100, Train loss: 149.4971, Val loss: 414.3154, Best Val loss: 414.3154\n",
            "Epoch 28/100, Train loss: 148.9619, Val loss: 397.2682, Best Val loss: 397.2682\n",
            "Epoch 29/100, Train loss: 134.9934, Val loss: 381.5875, Best Val loss: 381.5875\n",
            "Epoch 30/100, Train loss: 129.7716, Val loss: 367.0352, Best Val loss: 367.0352\n",
            "Epoch 31/100, Train loss: 122.9512, Val loss: 355.5418, Best Val loss: 355.5418\n",
            "Epoch 32/100, Train loss: 120.2475, Val loss: 343.1556, Best Val loss: 343.1556\n",
            "Epoch 33/100, Train loss: 115.7254, Val loss: 337.4571, Best Val loss: 337.4571\n",
            "Epoch 34/100, Train loss: 111.9262, Val loss: 326.3064, Best Val loss: 326.3064\n",
            "Epoch 35/100, Train loss: 107.2945, Val loss: 317.6145, Best Val loss: 317.6145\n",
            "Epoch 36/100, Train loss: 101.8822, Val loss: 306.7941, Best Val loss: 306.7941\n",
            "Epoch 37/100, Train loss: 98.6820, Val loss: 296.7818, Best Val loss: 296.7818\n",
            "Epoch 38/100, Train loss: 95.1657, Val loss: 287.1736, Best Val loss: 287.1736\n",
            "Epoch 39/100, Train loss: 100.7048, Val loss: 280.1674, Best Val loss: 280.1674\n",
            "Epoch 40/100, Train loss: 88.1887, Val loss: 270.8639, Best Val loss: 270.8639\n",
            "Epoch 41/100, Train loss: 91.7845, Val loss: 263.4495, Best Val loss: 263.4495\n",
            "Epoch 42/100, Train loss: 83.3187, Val loss: 256.6848, Best Val loss: 256.6848\n",
            "Epoch 43/100, Train loss: 80.3435, Val loss: 248.7605, Best Val loss: 248.7605\n",
            "Epoch 44/100, Train loss: 77.2635, Val loss: 242.9412, Best Val loss: 242.9412\n",
            "Epoch 45/100, Train loss: 83.7369, Val loss: 237.1792, Best Val loss: 237.1792\n",
            "Epoch 46/100, Train loss: 74.5032, Val loss: 230.8845, Best Val loss: 230.8845\n",
            "Epoch 47/100, Train loss: 71.2934, Val loss: 228.6478, Best Val loss: 228.6478\n",
            "Epoch 48/100, Train loss: 71.0341, Val loss: 225.8074, Best Val loss: 225.8074\n",
            "Epoch 49/100, Train loss: 70.5416, Val loss: 219.2958, Best Val loss: 219.2958\n",
            "Epoch 50/100, Train loss: 66.9632, Val loss: 217.2043, Best Val loss: 217.2043\n",
            "Epoch 51/100, Train loss: 66.2698, Val loss: 213.0280, Best Val loss: 213.0280\n",
            "Epoch 52/100, Train loss: 65.0341, Val loss: 208.8599, Best Val loss: 208.8599\n",
            "Epoch 53/100, Train loss: 63.2230, Val loss: 204.8267, Best Val loss: 204.8267\n",
            "Epoch 54/100, Train loss: 73.1040, Val loss: 201.0888, Best Val loss: 201.0888\n",
            "Epoch 55/100, Train loss: 66.1585, Val loss: 196.7682, Best Val loss: 196.7682\n",
            "Epoch 56/100, Train loss: 58.7034, Val loss: 192.7743, Best Val loss: 192.7743\n",
            "Epoch 57/100, Train loss: 58.3202, Val loss: 189.7817, Best Val loss: 189.7817\n",
            "Epoch 58/100, Train loss: 63.2943, Val loss: 186.0719, Best Val loss: 186.0719\n",
            "Epoch 59/100, Train loss: 59.5354, Val loss: 183.0814, Best Val loss: 183.0814\n",
            "Epoch 60/100, Train loss: 54.3823, Val loss: 180.1059, Best Val loss: 180.1059\n",
            "Epoch 61/100, Train loss: 52.8542, Val loss: 176.8038, Best Val loss: 176.8038\n",
            "Epoch 62/100, Train loss: 57.0694, Val loss: 174.6584, Best Val loss: 174.6584\n",
            "Epoch 63/100, Train loss: 50.9153, Val loss: 171.9722, Best Val loss: 171.9722\n",
            "Epoch 64/100, Train loss: 50.1570, Val loss: 169.4595, Best Val loss: 169.4595\n",
            "Epoch 65/100, Train loss: 57.7392, Val loss: 167.4912, Best Val loss: 167.4912\n",
            "Epoch 66/100, Train loss: 48.4486, Val loss: 165.2902, Best Val loss: 165.2902\n",
            "Epoch 67/100, Train loss: 47.5516, Val loss: 163.3420, Best Val loss: 163.3420\n",
            "Epoch 68/100, Train loss: 49.2988, Val loss: 161.4535, Best Val loss: 161.4535\n",
            "Epoch 69/100, Train loss: 46.5157, Val loss: 159.7875, Best Val loss: 159.7875\n",
            "Epoch 70/100, Train loss: 46.8554, Val loss: 157.9164, Best Val loss: 157.9164\n",
            "Epoch 71/100, Train loss: 45.3385, Val loss: 156.8835, Best Val loss: 156.8835\n",
            "Epoch 72/100, Train loss: 44.6407, Val loss: 155.1018, Best Val loss: 155.1018\n",
            "Epoch 73/100, Train loss: 45.7243, Val loss: 154.0747, Best Val loss: 154.0747\n",
            "Epoch 74/100, Train loss: 43.5764, Val loss: 152.8205, Best Val loss: 152.8205\n",
            "Epoch 75/100, Train loss: 43.4787, Val loss: 152.0210, Best Val loss: 152.0210\n",
            "Epoch 76/100, Train loss: 47.1447, Val loss: 150.6820, Best Val loss: 150.6820\n",
            "Epoch 77/100, Train loss: 42.5366, Val loss: 149.9385, Best Val loss: 149.9385\n",
            "Epoch 78/100, Train loss: 50.0676, Val loss: 148.9580, Best Val loss: 148.9580\n",
            "Epoch 79/100, Train loss: 41.8627, Val loss: 148.0077, Best Val loss: 148.0077\n",
            "Epoch 80/100, Train loss: 44.7461, Val loss: 147.4289, Best Val loss: 147.4289\n",
            "Epoch 81/100, Train loss: 41.2808, Val loss: 146.7014, Best Val loss: 146.7014\n",
            "Epoch 82/100, Train loss: 41.0375, Val loss: 146.0394, Best Val loss: 146.0394\n",
            "Epoch 83/100, Train loss: 40.8330, Val loss: 145.3564, Best Val loss: 145.3564\n",
            "Epoch 84/100, Train loss: 40.8789, Val loss: 144.9501, Best Val loss: 144.9501\n",
            "Epoch 85/100, Train loss: 40.6990, Val loss: 144.5531, Best Val loss: 144.5531\n",
            "Epoch 86/100, Train loss: 40.4930, Val loss: 144.0742, Best Val loss: 144.0742\n",
            "Epoch 87/100, Train loss: 40.1872, Val loss: 143.5773, Best Val loss: 143.5773\n",
            "Epoch 88/100, Train loss: 46.4982, Val loss: 143.2656, Best Val loss: 143.2656\n",
            "Epoch 89/100, Train loss: 39.9350, Val loss: 142.9936, Best Val loss: 142.9936\n",
            "Epoch 90/100, Train loss: 39.8306, Val loss: 142.7460, Best Val loss: 142.7460\n",
            "Epoch 91/100, Train loss: 39.7465, Val loss: 142.5657, Best Val loss: 142.5657\n",
            "Epoch 92/100, Train loss: 39.6863, Val loss: 142.4083, Best Val loss: 142.4083\n",
            "Epoch 93/100, Train loss: 43.0191, Val loss: 142.2866, Best Val loss: 142.2866\n",
            "Epoch 94/100, Train loss: 39.8381, Val loss: 142.2008, Best Val loss: 142.2008\n",
            "Epoch 95/100, Train loss: 42.7816, Val loss: 142.1518, Best Val loss: 142.1518\n",
            "Epoch 96/100, Train loss: 39.7841, Val loss: 142.0954, Best Val loss: 142.0954\n",
            "Epoch 97/100, Train loss: 39.5914, Val loss: 142.0664, Best Val loss: 142.0664\n",
            "Epoch 98/100, Train loss: 39.7631, Val loss: 142.0475, Best Val loss: 142.0475\n",
            "Epoch 99/100, Train loss: 40.0508, Val loss: 142.0416, Best Val loss: 142.0416\n",
            "Epoch 100/100, Train loss: 39.5185, Val loss: 142.0397, Best Val loss: 142.0397\n",
            "Epoch 1/100, Train loss: 1028.0434, Val loss: 1633.2198, Best Val loss: 1633.2198\n",
            "Epoch 2/100, Train loss: 801.6179, Val loss: 1476.1473, Best Val loss: 1476.1473\n",
            "Epoch 3/100, Train loss: 717.4167, Val loss: 1361.6056, Best Val loss: 1361.6056\n",
            "Epoch 4/100, Train loss: 646.0907, Val loss: 1281.7415, Best Val loss: 1281.7415\n",
            "Epoch 5/100, Train loss: 619.6580, Val loss: 1208.0656, Best Val loss: 1208.0656\n",
            "Epoch 6/100, Train loss: 548.8622, Val loss: 1138.2039, Best Val loss: 1138.2039\n",
            "Epoch 7/100, Train loss: 526.4362, Val loss: 1082.6274, Best Val loss: 1082.6274\n",
            "Epoch 8/100, Train loss: 488.8766, Val loss: 1027.0396, Best Val loss: 1027.0396\n",
            "Epoch 9/100, Train loss: 427.9940, Val loss: 975.4852, Best Val loss: 975.4852\n",
            "Epoch 10/100, Train loss: 391.8173, Val loss: 928.5230, Best Val loss: 928.5230\n",
            "Epoch 11/100, Train loss: 372.6306, Val loss: 884.1470, Best Val loss: 884.1470\n",
            "Epoch 12/100, Train loss: 369.5718, Val loss: 844.1644, Best Val loss: 844.1644\n",
            "Epoch 13/100, Train loss: 336.2945, Val loss: 803.9073, Best Val loss: 803.9073\n",
            "Epoch 14/100, Train loss: 318.9183, Val loss: 765.3074, Best Val loss: 765.3074\n",
            "Epoch 15/100, Train loss: 307.0780, Val loss: 728.5204, Best Val loss: 728.5204\n",
            "Epoch 16/100, Train loss: 263.7096, Val loss: 693.0600, Best Val loss: 693.0600\n",
            "Epoch 17/100, Train loss: 247.1314, Val loss: 662.0618, Best Val loss: 662.0618\n",
            "Epoch 18/100, Train loss: 251.9581, Val loss: 635.0712, Best Val loss: 635.0712\n",
            "Epoch 19/100, Train loss: 238.2623, Val loss: 609.1217, Best Val loss: 609.1217\n",
            "Epoch 20/100, Train loss: 207.5979, Val loss: 580.0453, Best Val loss: 580.0453\n",
            "Epoch 21/100, Train loss: 194.9370, Val loss: 555.4098, Best Val loss: 555.4098\n",
            "Epoch 22/100, Train loss: 184.3910, Val loss: 532.0151, Best Val loss: 532.0151\n",
            "Epoch 23/100, Train loss: 186.7514, Val loss: 511.4221, Best Val loss: 511.4221\n",
            "Epoch 24/100, Train loss: 164.9583, Val loss: 490.2118, Best Val loss: 490.2118\n",
            "Epoch 25/100, Train loss: 155.8257, Val loss: 473.0978, Best Val loss: 473.0978\n",
            "Epoch 26/100, Train loss: 149.1708, Val loss: 455.0665, Best Val loss: 455.0665\n",
            "Epoch 27/100, Train loss: 141.0233, Val loss: 437.3991, Best Val loss: 437.3991\n",
            "Epoch 28/100, Train loss: 141.0568, Val loss: 420.3506, Best Val loss: 420.3506\n",
            "Epoch 29/100, Train loss: 127.3571, Val loss: 404.7169, Best Val loss: 404.7169\n",
            "Epoch 30/100, Train loss: 122.9544, Val loss: 389.0047, Best Val loss: 389.0047\n",
            "Epoch 31/100, Train loss: 115.2725, Val loss: 377.2926, Best Val loss: 377.2926\n",
            "Epoch 32/100, Train loss: 111.7376, Val loss: 363.9132, Best Val loss: 363.9132\n",
            "Epoch 33/100, Train loss: 105.0238, Val loss: 350.8325, Best Val loss: 350.8325\n",
            "Epoch 34/100, Train loss: 100.3949, Val loss: 338.8816, Best Val loss: 338.8816\n",
            "Epoch 35/100, Train loss: 96.7409, Val loss: 327.2185, Best Val loss: 327.2185\n",
            "Epoch 36/100, Train loss: 91.8939, Val loss: 318.3356, Best Val loss: 318.3356\n",
            "Epoch 37/100, Train loss: 88.8709, Val loss: 308.8812, Best Val loss: 308.8812\n",
            "Epoch 38/100, Train loss: 85.8638, Val loss: 299.5122, Best Val loss: 299.5122\n",
            "Epoch 39/100, Train loss: 91.0942, Val loss: 292.4621, Best Val loss: 292.4621\n",
            "Epoch 40/100, Train loss: 79.4312, Val loss: 283.6685, Best Val loss: 283.6685\n",
            "Epoch 41/100, Train loss: 84.4934, Val loss: 275.6467, Best Val loss: 275.6467\n",
            "Epoch 42/100, Train loss: 74.4550, Val loss: 270.2184, Best Val loss: 270.2184\n",
            "Epoch 43/100, Train loss: 73.0752, Val loss: 260.8672, Best Val loss: 260.8672\n",
            "Epoch 44/100, Train loss: 69.4970, Val loss: 255.2135, Best Val loss: 255.2135\n",
            "Epoch 45/100, Train loss: 73.8252, Val loss: 249.3383, Best Val loss: 249.3383\n",
            "Epoch 46/100, Train loss: 65.9533, Val loss: 243.3434, Best Val loss: 243.3434\n",
            "Epoch 47/100, Train loss: 64.4183, Val loss: 237.5004, Best Val loss: 237.5004\n",
            "Epoch 48/100, Train loss: 61.8173, Val loss: 232.2187, Best Val loss: 232.2187\n",
            "Epoch 49/100, Train loss: 61.2478, Val loss: 227.8238, Best Val loss: 227.8238\n",
            "Epoch 50/100, Train loss: 58.6894, Val loss: 223.0754, Best Val loss: 223.0754\n",
            "Epoch 51/100, Train loss: 57.5292, Val loss: 218.5171, Best Val loss: 218.5171\n",
            "Epoch 52/100, Train loss: 56.5286, Val loss: 217.7110, Best Val loss: 217.7110\n",
            "Epoch 53/100, Train loss: 55.2581, Val loss: 213.9877, Best Val loss: 213.9877\n",
            "Epoch 54/100, Train loss: 61.9443, Val loss: 210.4890, Best Val loss: 210.4890\n",
            "Epoch 55/100, Train loss: 57.5920, Val loss: 206.1356, Best Val loss: 206.1356\n",
            "Epoch 56/100, Train loss: 51.4878, Val loss: 202.2554, Best Val loss: 202.2554\n",
            "Epoch 57/100, Train loss: 49.9949, Val loss: 198.7957, Best Val loss: 198.7957\n",
            "Epoch 58/100, Train loss: 54.4551, Val loss: 195.3826, Best Val loss: 195.3826\n",
            "Epoch 59/100, Train loss: 51.6382, Val loss: 192.1970, Best Val loss: 192.1970\n",
            "Epoch 60/100, Train loss: 46.8770, Val loss: 189.2123, Best Val loss: 189.2123\n",
            "Epoch 61/100, Train loss: 46.1541, Val loss: 186.1355, Best Val loss: 186.1355\n",
            "Epoch 62/100, Train loss: 50.8809, Val loss: 184.2045, Best Val loss: 184.2045\n",
            "Epoch 63/100, Train loss: 44.3884, Val loss: 181.1947, Best Val loss: 181.1947\n",
            "Epoch 64/100, Train loss: 43.3968, Val loss: 178.5239, Best Val loss: 178.5239\n",
            "Epoch 65/100, Train loss: 50.2713, Val loss: 176.5721, Best Val loss: 176.5721\n",
            "Epoch 66/100, Train loss: 41.7569, Val loss: 174.3634, Best Val loss: 174.3634\n",
            "Epoch 67/100, Train loss: 41.1612, Val loss: 171.8128, Best Val loss: 171.8128\n",
            "Epoch 68/100, Train loss: 42.4510, Val loss: 170.3347, Best Val loss: 170.3347\n",
            "Epoch 69/100, Train loss: 40.0912, Val loss: 168.0805, Best Val loss: 168.0805\n",
            "Epoch 70/100, Train loss: 39.8586, Val loss: 166.5229, Best Val loss: 166.5229\n",
            "Epoch 71/100, Train loss: 39.0815, Val loss: 165.2235, Best Val loss: 165.2235\n",
            "Epoch 72/100, Train loss: 38.4271, Val loss: 163.8640, Best Val loss: 163.8640\n",
            "Epoch 73/100, Train loss: 39.7571, Val loss: 162.7391, Best Val loss: 162.7391\n",
            "Epoch 74/100, Train loss: 37.4979, Val loss: 161.5375, Best Val loss: 161.5375\n",
            "Epoch 75/100, Train loss: 37.1983, Val loss: 160.3931, Best Val loss: 160.3931\n",
            "Epoch 76/100, Train loss: 40.3170, Val loss: 159.3200, Best Val loss: 159.3200\n",
            "Epoch 77/100, Train loss: 36.4424, Val loss: 158.3439, Best Val loss: 158.3439\n",
            "Epoch 78/100, Train loss: 43.7946, Val loss: 157.3869, Best Val loss: 157.3869\n",
            "Epoch 79/100, Train loss: 35.9048, Val loss: 156.4360, Best Val loss: 156.4360\n",
            "Epoch 80/100, Train loss: 38.7756, Val loss: 155.8077, Best Val loss: 155.8077\n",
            "Epoch 81/100, Train loss: 35.3515, Val loss: 155.0318, Best Val loss: 155.0318\n",
            "Epoch 82/100, Train loss: 35.1756, Val loss: 154.4005, Best Val loss: 154.4005\n",
            "Epoch 83/100, Train loss: 35.0307, Val loss: 153.8506, Best Val loss: 153.8506\n",
            "Epoch 84/100, Train loss: 34.8334, Val loss: 153.2895, Best Val loss: 153.2895\n",
            "Epoch 85/100, Train loss: 34.6774, Val loss: 152.9859, Best Val loss: 152.9859\n",
            "Epoch 86/100, Train loss: 34.5554, Val loss: 152.4804, Best Val loss: 152.4804\n",
            "Epoch 87/100, Train loss: 34.4191, Val loss: 152.1443, Best Val loss: 152.1443\n",
            "Epoch 88/100, Train loss: 41.3284, Val loss: 151.9145, Best Val loss: 151.9145\n",
            "Epoch 89/100, Train loss: 34.2316, Val loss: 151.6531, Best Val loss: 151.6531\n",
            "Epoch 90/100, Train loss: 34.1646, Val loss: 151.3837, Best Val loss: 151.3837\n",
            "Epoch 91/100, Train loss: 34.0912, Val loss: 151.1834, Best Val loss: 151.1834\n",
            "Epoch 92/100, Train loss: 34.0406, Val loss: 151.0386, Best Val loss: 151.0386\n",
            "Epoch 93/100, Train loss: 37.9113, Val loss: 150.9433, Best Val loss: 150.9433\n",
            "Epoch 94/100, Train loss: 33.9595, Val loss: 150.8361, Best Val loss: 150.8361\n",
            "Epoch 95/100, Train loss: 36.5606, Val loss: 150.7866, Best Val loss: 150.7866\n",
            "Epoch 96/100, Train loss: 33.9136, Val loss: 150.7300, Best Val loss: 150.7300\n",
            "Epoch 97/100, Train loss: 33.9062, Val loss: 150.6915, Best Val loss: 150.6915\n",
            "Epoch 98/100, Train loss: 33.8968, Val loss: 150.6774, Best Val loss: 150.6774\n",
            "Epoch 99/100, Train loss: 34.1135, Val loss: 150.6682, Best Val loss: 150.6682\n",
            "Epoch 100/100, Train loss: 33.8860, Val loss: 150.6670, Best Val loss: 150.6670\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Window Size  Step Size  MSE (Raw Input)\n",
              "0           10          5        17.669674\n",
              "1           10         10       159.790543\n",
              "2            5         10       161.606033"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9fc7205b-fba2-451e-b5c2-5e89e34cb51d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Window Size</th>\n",
              "      <th>Step Size</th>\n",
              "      <th>MSE (Raw Input)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>17.669674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>159.790543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>161.606033</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9fc7205b-fba2-451e-b5c2-5e89e34cb51d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9fc7205b-fba2-451e-b5c2-5e89e34cb51d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9fc7205b-fba2-451e-b5c2-5e89e34cb51d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8c6bc67a-e697-4aca-a03a-be28fe774165\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8c6bc67a-e697-4aca-a03a-be28fe774165')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8c6bc67a-e697-4aca-a03a-be28fe774165 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Window Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 5,\n        \"max\": 10,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          5,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Step Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 5,\n        \"max\": 10,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          10,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MSE (Raw Input)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          17.669673919677734,\n          159.79054260253906\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **加入Volume欄位**"
      ],
      "metadata": {
        "id": "TbJygZmryk1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = [\n",
        "    (10, 5),   # window > step\n",
        "    (10, 10),  # window = step\n",
        "    (5, 10)    # window < step\n",
        "]\n",
        "results = []\n",
        "\n",
        "for w, s in params:\n",
        "    mse = run_experiment(df, window_size=w, step=s, with_volume=True)\n",
        "    results.append({'Window Size': w, 'Step Size': s, 'MSE (Raw Input)': mse})\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yDpRVpkSoLC6",
        "outputId": "e8ee3d25-20db-45a6-d139-06220ae44f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train loss: 1400.4708, Val loss: 961.2421, Best Val loss: 961.2421\n",
            "Epoch 2/100, Train loss: 1254.8013, Val loss: 892.3381, Best Val loss: 892.3381\n",
            "Epoch 3/100, Train loss: 1186.7337, Val loss: 837.7018, Best Val loss: 837.7018\n",
            "Epoch 4/100, Train loss: 1128.8326, Val loss: 794.5156, Best Val loss: 794.5156\n",
            "Epoch 5/100, Train loss: 1136.4621, Val loss: 762.5956, Best Val loss: 762.5956\n",
            "Epoch 6/100, Train loss: 1042.2740, Val loss: 736.0078, Best Val loss: 736.0078\n",
            "Epoch 7/100, Train loss: 1116.0006, Val loss: 719.9597, Best Val loss: 719.9597\n",
            "Epoch 8/100, Train loss: 988.4147, Val loss: 706.1372, Best Val loss: 706.1372\n",
            "Epoch 9/100, Train loss: 971.5909, Val loss: 697.8559, Best Val loss: 697.8559\n",
            "Epoch 10/100, Train loss: 1145.4332, Val loss: 692.9116, Best Val loss: 692.9116\n",
            "Epoch 11/100, Train loss: 951.4209, Val loss: 689.6803, Best Val loss: 689.6803\n",
            "Epoch 12/100, Train loss: 943.0079, Val loss: 688.3216, Best Val loss: 688.3216\n",
            "Epoch 13/100, Train loss: 1008.7102, Val loss: 688.0310, Best Val loss: 688.0310\n",
            "Epoch 14/100, Train loss: 1180.0466, Val loss: 688.3146, Best Val loss: 688.0310\n",
            "Epoch 15/100, Train loss: 941.5412, Val loss: 689.5053, Best Val loss: 688.0310\n",
            "Epoch 16/100, Train loss: 1035.5678, Val loss: 690.4302, Best Val loss: 688.0310\n",
            "Epoch 17/100, Train loss: 1066.4682, Val loss: 691.7273, Best Val loss: 688.0310\n",
            "Epoch 18/100, Train loss: 933.3355, Val loss: 693.2117, Best Val loss: 688.0310\n",
            "Epoch 19/100, Train loss: 931.9164, Val loss: 693.1042, Best Val loss: 688.0310\n",
            "Epoch 20/100, Train loss: 933.4544, Val loss: 693.3158, Best Val loss: 688.0310\n",
            "Epoch 21/100, Train loss: 931.8357, Val loss: 693.4857, Best Val loss: 688.0310\n",
            "Epoch 22/100, Train loss: 933.7879, Val loss: 693.7037, Best Val loss: 688.0310\n",
            "Epoch 23/100, Train loss: 937.1618, Val loss: 693.6974, Best Val loss: 688.0310\n",
            "Epoch 24/100, Train loss: 934.1089, Val loss: 694.6087, Best Val loss: 688.0310\n",
            "Epoch 25/100, Train loss: 931.7685, Val loss: 694.3379, Best Val loss: 688.0310\n",
            "Epoch 26/100, Train loss: 930.7759, Val loss: 694.1914, Best Val loss: 688.0310\n",
            "Epoch 27/100, Train loss: 951.4440, Val loss: 694.7167, Best Val loss: 688.0310\n",
            "Epoch 28/100, Train loss: 930.4301, Val loss: 694.8551, Best Val loss: 688.0310\n",
            "Epoch 29/100, Train loss: 1029.9992, Val loss: 694.8518, Best Val loss: 688.0310\n",
            "Epoch 30/100, Train loss: 932.6789, Val loss: 695.1833, Best Val loss: 688.0310\n",
            "Epoch 31/100, Train loss: 931.3070, Val loss: 695.6774, Best Val loss: 688.0310\n",
            "Epoch 32/100, Train loss: 936.1924, Val loss: 695.3184, Best Val loss: 688.0310\n",
            "Epoch 33/100, Train loss: 932.8969, Val loss: 695.1269, Best Val loss: 688.0310\n",
            "Epoch 34/100, Train loss: 932.8136, Val loss: 695.0490, Best Val loss: 688.0310\n",
            "Epoch 35/100, Train loss: 953.6046, Val loss: 694.9221, Best Val loss: 688.0310\n",
            "Epoch 36/100, Train loss: 1085.5465, Val loss: 694.6843, Best Val loss: 688.0310\n",
            "Epoch 37/100, Train loss: 930.4239, Val loss: 696.9558, Best Val loss: 688.0310\n",
            "Epoch 38/100, Train loss: 999.7000, Val loss: 695.8217, Best Val loss: 688.0310\n",
            "Epoch 39/100, Train loss: 931.6788, Val loss: 696.6523, Best Val loss: 688.0310\n",
            "Epoch 40/100, Train loss: 1133.0052, Val loss: 696.4018, Best Val loss: 688.0310\n",
            "Epoch 41/100, Train loss: 1032.2340, Val loss: 696.9525, Best Val loss: 688.0310\n",
            "Epoch 42/100, Train loss: 932.3335, Val loss: 697.6449, Best Val loss: 688.0310\n",
            "Epoch 43/100, Train loss: 1029.3789, Val loss: 697.1027, Best Val loss: 688.0310\n",
            "Epoch 44/100, Train loss: 961.7820, Val loss: 697.0775, Best Val loss: 688.0310\n",
            "Epoch 45/100, Train loss: 931.1182, Val loss: 696.8690, Best Val loss: 688.0310\n",
            "Epoch 46/100, Train loss: 931.4030, Val loss: 696.8036, Best Val loss: 688.0310\n",
            "Epoch 47/100, Train loss: 953.7865, Val loss: 695.8317, Best Val loss: 688.0310\n",
            "Epoch 48/100, Train loss: 931.4671, Val loss: 695.8001, Best Val loss: 688.0310\n",
            "Epoch 49/100, Train loss: 939.7858, Val loss: 695.7848, Best Val loss: 688.0310\n",
            "Epoch 50/100, Train loss: 1064.8969, Val loss: 695.7834, Best Val loss: 688.0310\n",
            "Epoch 51/100, Train loss: 962.1520, Val loss: 696.5642, Best Val loss: 688.0310\n",
            "Epoch 52/100, Train loss: 931.4713, Val loss: 696.9094, Best Val loss: 688.0310\n",
            "Epoch 53/100, Train loss: 1021.7384, Val loss: 696.9639, Best Val loss: 688.0310\n",
            "Epoch 54/100, Train loss: 929.8641, Val loss: 696.6521, Best Val loss: 688.0310\n",
            "Epoch 55/100, Train loss: 993.7610, Val loss: 696.3594, Best Val loss: 688.0310\n",
            "Epoch 56/100, Train loss: 935.0088, Val loss: 696.5155, Best Val loss: 688.0310\n",
            "Epoch 57/100, Train loss: 954.6288, Val loss: 696.3669, Best Val loss: 688.0310\n",
            "Epoch 58/100, Train loss: 999.4535, Val loss: 696.4472, Best Val loss: 688.0310\n",
            "Epoch 59/100, Train loss: 938.4099, Val loss: 696.4216, Best Val loss: 688.0310\n",
            "Epoch 60/100, Train loss: 930.8260, Val loss: 697.0400, Best Val loss: 688.0310\n",
            "Epoch 61/100, Train loss: 933.8024, Val loss: 696.3462, Best Val loss: 688.0310\n",
            "Epoch 62/100, Train loss: 929.8457, Val loss: 696.1925, Best Val loss: 688.0310\n",
            "Epoch 63/100, Train loss: 932.6499, Val loss: 696.0134, Best Val loss: 688.0310\n",
            "Epoch 64/100, Train loss: 932.1491, Val loss: 695.7931, Best Val loss: 688.0310\n",
            "Epoch 65/100, Train loss: 929.7251, Val loss: 695.6913, Best Val loss: 688.0310\n",
            "Epoch 66/100, Train loss: 932.0465, Val loss: 695.6485, Best Val loss: 688.0310\n",
            "Epoch 67/100, Train loss: 941.1123, Val loss: 695.4538, Best Val loss: 688.0310\n",
            "Epoch 68/100, Train loss: 986.0990, Val loss: 695.5656, Best Val loss: 688.0310\n",
            "Epoch 69/100, Train loss: 932.6499, Val loss: 695.7361, Best Val loss: 688.0310\n",
            "Epoch 70/100, Train loss: 961.9465, Val loss: 695.6531, Best Val loss: 688.0310\n",
            "Epoch 71/100, Train loss: 943.1202, Val loss: 695.7589, Best Val loss: 688.0310\n",
            "Epoch 72/100, Train loss: 937.0095, Val loss: 695.8891, Best Val loss: 688.0310\n",
            "Epoch 73/100, Train loss: 930.2605, Val loss: 695.7040, Best Val loss: 688.0310\n",
            "Epoch 74/100, Train loss: 932.6507, Val loss: 695.6307, Best Val loss: 688.0310\n",
            "Epoch 75/100, Train loss: 957.7113, Val loss: 695.6723, Best Val loss: 688.0310\n",
            "Epoch 76/100, Train loss: 932.1273, Val loss: 695.6743, Best Val loss: 688.0310\n",
            "Epoch 77/100, Train loss: 930.2493, Val loss: 695.6784, Best Val loss: 688.0310\n",
            "Epoch 78/100, Train loss: 934.2521, Val loss: 695.6051, Best Val loss: 688.0310\n",
            "Epoch 79/100, Train loss: 933.3804, Val loss: 695.6206, Best Val loss: 688.0310\n",
            "Epoch 80/100, Train loss: 977.7311, Val loss: 695.5519, Best Val loss: 688.0310\n",
            "Epoch 81/100, Train loss: 1000.5342, Val loss: 695.5598, Best Val loss: 688.0310\n",
            "Epoch 82/100, Train loss: 930.0910, Val loss: 695.7081, Best Val loss: 688.0310\n",
            "Epoch 83/100, Train loss: 1020.1868, Val loss: 695.6934, Best Val loss: 688.0310\n",
            "Epoch 84/100, Train loss: 1023.2226, Val loss: 695.7402, Best Val loss: 688.0310\n",
            "Epoch 85/100, Train loss: 958.8766, Val loss: 695.8271, Best Val loss: 688.0310\n",
            "Epoch 86/100, Train loss: 1098.5638, Val loss: 695.7468, Best Val loss: 688.0310\n",
            "Epoch 87/100, Train loss: 961.4313, Val loss: 695.9038, Best Val loss: 688.0310\n",
            "Epoch 88/100, Train loss: 972.8334, Val loss: 695.9500, Best Val loss: 688.0310\n",
            "Epoch 89/100, Train loss: 1023.1578, Val loss: 695.9336, Best Val loss: 688.0310\n",
            "Epoch 90/100, Train loss: 931.4177, Val loss: 695.9419, Best Val loss: 688.0310\n",
            "Epoch 91/100, Train loss: 933.6378, Val loss: 695.9547, Best Val loss: 688.0310\n",
            "Epoch 92/100, Train loss: 931.1117, Val loss: 695.9358, Best Val loss: 688.0310\n",
            "Epoch 93/100, Train loss: 937.9705, Val loss: 695.9217, Best Val loss: 688.0310\n",
            "Epoch 94/100, Train loss: 933.2680, Val loss: 695.9211, Best Val loss: 688.0310\n",
            "Epoch 95/100, Train loss: 931.2425, Val loss: 695.9210, Best Val loss: 688.0310\n",
            "Epoch 96/100, Train loss: 962.6139, Val loss: 695.9168, Best Val loss: 688.0310\n",
            "Epoch 97/100, Train loss: 931.6460, Val loss: 695.9169, Best Val loss: 688.0310\n",
            "Epoch 98/100, Train loss: 1080.2156, Val loss: 695.9140, Best Val loss: 688.0310\n",
            "Epoch 99/100, Train loss: 933.4648, Val loss: 695.9166, Best Val loss: 688.0310\n",
            "Epoch 100/100, Train loss: 1149.9150, Val loss: 695.9165, Best Val loss: 688.0310\n",
            "Epoch 1/100, Train loss: 1158.4355, Val loss: 1879.1635, Best Val loss: 1879.1635\n",
            "Epoch 2/100, Train loss: 1162.5182, Val loss: 1828.9542, Best Val loss: 1828.9542\n",
            "Epoch 3/100, Train loss: 1164.1884, Val loss: 1782.4108, Best Val loss: 1782.4108\n",
            "Epoch 4/100, Train loss: 1079.4436, Val loss: 1739.0352, Best Val loss: 1739.0352\n",
            "Epoch 5/100, Train loss: 1086.9617, Val loss: 1700.7170, Best Val loss: 1700.7170\n",
            "Epoch 6/100, Train loss: 1007.1123, Val loss: 1665.1489, Best Val loss: 1665.1489\n",
            "Epoch 7/100, Train loss: 996.9118, Val loss: 1633.7624, Best Val loss: 1633.7624\n",
            "Epoch 8/100, Train loss: 906.7911, Val loss: 1604.5627, Best Val loss: 1604.5627\n",
            "Epoch 9/100, Train loss: 910.5325, Val loss: 1580.3503, Best Val loss: 1580.3503\n",
            "Epoch 10/100, Train loss: 867.2993, Val loss: 1557.4499, Best Val loss: 1557.4499\n",
            "Epoch 11/100, Train loss: 858.2590, Val loss: 1536.8966, Best Val loss: 1536.8966\n",
            "Epoch 12/100, Train loss: 883.8485, Val loss: 1519.7824, Best Val loss: 1519.7824\n",
            "Epoch 13/100, Train loss: 840.1562, Val loss: 1503.1503, Best Val loss: 1503.1503\n",
            "Epoch 14/100, Train loss: 832.5501, Val loss: 1489.4580, Best Val loss: 1489.4580\n",
            "Epoch 15/100, Train loss: 846.6512, Val loss: 1476.7006, Best Val loss: 1476.7006\n",
            "Epoch 16/100, Train loss: 807.3723, Val loss: 1465.8401, Best Val loss: 1465.8401\n",
            "Epoch 17/100, Train loss: 792.6639, Val loss: 1455.9959, Best Val loss: 1455.9959\n",
            "Epoch 18/100, Train loss: 881.8893, Val loss: 1448.8680, Best Val loss: 1448.8680\n",
            "Epoch 19/100, Train loss: 784.8469, Val loss: 1440.7736, Best Val loss: 1440.7736\n",
            "Epoch 20/100, Train loss: 787.0257, Val loss: 1434.5311, Best Val loss: 1434.5311\n",
            "Epoch 21/100, Train loss: 827.1634, Val loss: 1428.4129, Best Val loss: 1428.4129\n",
            "Epoch 22/100, Train loss: 778.9812, Val loss: 1423.0131, Best Val loss: 1423.0131\n",
            "Epoch 23/100, Train loss: 786.8464, Val loss: 1418.7465, Best Val loss: 1418.7465\n",
            "Epoch 24/100, Train loss: 774.5628, Val loss: 1414.9857, Best Val loss: 1414.9857\n",
            "Epoch 25/100, Train loss: 785.4743, Val loss: 1412.1488, Best Val loss: 1412.1488\n",
            "Epoch 26/100, Train loss: 804.2053, Val loss: 1408.9135, Best Val loss: 1408.9135\n",
            "Epoch 27/100, Train loss: 812.6778, Val loss: 1406.3182, Best Val loss: 1406.3182\n",
            "Epoch 28/100, Train loss: 775.8098, Val loss: 1404.1810, Best Val loss: 1404.1810\n",
            "Epoch 29/100, Train loss: 825.6592, Val loss: 1402.0558, Best Val loss: 1402.0558\n",
            "Epoch 30/100, Train loss: 776.0851, Val loss: 1400.3630, Best Val loss: 1400.3630\n",
            "Epoch 31/100, Train loss: 762.7828, Val loss: 1398.9721, Best Val loss: 1398.9721\n",
            "Epoch 32/100, Train loss: 810.2776, Val loss: 1397.6100, Best Val loss: 1397.6100\n",
            "Epoch 33/100, Train loss: 759.8576, Val loss: 1396.0563, Best Val loss: 1396.0563\n",
            "Epoch 34/100, Train loss: 770.8934, Val loss: 1395.5496, Best Val loss: 1395.5496\n",
            "Epoch 35/100, Train loss: 819.1454, Val loss: 1394.5433, Best Val loss: 1394.5433\n",
            "Epoch 36/100, Train loss: 761.6877, Val loss: 1393.7999, Best Val loss: 1393.7999\n",
            "Epoch 37/100, Train loss: 758.9887, Val loss: 1393.2086, Best Val loss: 1393.2086\n",
            "Epoch 38/100, Train loss: 798.0969, Val loss: 1392.6529, Best Val loss: 1392.6529\n",
            "Epoch 39/100, Train loss: 811.7831, Val loss: 1392.2837, Best Val loss: 1392.2837\n",
            "Epoch 40/100, Train loss: 761.7370, Val loss: 1391.7827, Best Val loss: 1391.7827\n",
            "Epoch 41/100, Train loss: 792.3795, Val loss: 1391.3754, Best Val loss: 1391.3754\n",
            "Epoch 42/100, Train loss: 793.8481, Val loss: 1391.0224, Best Val loss: 1391.0224\n",
            "Epoch 43/100, Train loss: 779.7750, Val loss: 1390.6947, Best Val loss: 1390.6947\n",
            "Epoch 44/100, Train loss: 765.5038, Val loss: 1390.3652, Best Val loss: 1390.3652\n",
            "Epoch 45/100, Train loss: 821.2079, Val loss: 1390.1050, Best Val loss: 1390.1050\n",
            "Epoch 46/100, Train loss: 758.3646, Val loss: 1389.6652, Best Val loss: 1389.6652\n",
            "Epoch 47/100, Train loss: 769.4495, Val loss: 1389.4853, Best Val loss: 1389.4853\n",
            "Epoch 48/100, Train loss: 756.8942, Val loss: 1389.2896, Best Val loss: 1389.2896\n",
            "Epoch 49/100, Train loss: 760.8845, Val loss: 1389.3941, Best Val loss: 1389.2896\n",
            "Epoch 50/100, Train loss: 758.0464, Val loss: 1389.1058, Best Val loss: 1389.1058\n",
            "Epoch 51/100, Train loss: 830.5772, Val loss: 1389.1109, Best Val loss: 1389.1058\n",
            "Epoch 52/100, Train loss: 757.9487, Val loss: 1389.0685, Best Val loss: 1389.0685\n",
            "Epoch 53/100, Train loss: 801.6489, Val loss: 1388.9275, Best Val loss: 1388.9275\n",
            "Epoch 54/100, Train loss: 769.1108, Val loss: 1388.7363, Best Val loss: 1388.7363\n",
            "Epoch 55/100, Train loss: 797.2301, Val loss: 1389.0029, Best Val loss: 1388.7363\n",
            "Epoch 56/100, Train loss: 808.5023, Val loss: 1388.6794, Best Val loss: 1388.6794\n",
            "Epoch 57/100, Train loss: 794.1977, Val loss: 1388.6326, Best Val loss: 1388.6326\n",
            "Epoch 58/100, Train loss: 765.5518, Val loss: 1388.4018, Best Val loss: 1388.4018\n",
            "Epoch 59/100, Train loss: 816.0771, Val loss: 1388.6506, Best Val loss: 1388.4018\n",
            "Epoch 60/100, Train loss: 756.2798, Val loss: 1388.2324, Best Val loss: 1388.2324\n",
            "Epoch 61/100, Train loss: 756.9311, Val loss: 1388.0403, Best Val loss: 1388.0403\n",
            "Epoch 62/100, Train loss: 774.9234, Val loss: 1388.1097, Best Val loss: 1388.0403\n",
            "Epoch 63/100, Train loss: 757.7008, Val loss: 1388.1513, Best Val loss: 1388.0403\n",
            "Epoch 64/100, Train loss: 761.6257, Val loss: 1388.1593, Best Val loss: 1388.0403\n",
            "Epoch 65/100, Train loss: 838.4093, Val loss: 1388.2015, Best Val loss: 1388.0403\n",
            "Epoch 66/100, Train loss: 768.7940, Val loss: 1388.1612, Best Val loss: 1388.0403\n",
            "Epoch 67/100, Train loss: 770.5771, Val loss: 1388.1013, Best Val loss: 1388.0403\n",
            "Epoch 68/100, Train loss: 812.7996, Val loss: 1387.9851, Best Val loss: 1387.9851\n",
            "Epoch 69/100, Train loss: 760.3588, Val loss: 1387.9083, Best Val loss: 1387.9083\n",
            "Epoch 70/100, Train loss: 761.3254, Val loss: 1387.9194, Best Val loss: 1387.9083\n",
            "Epoch 71/100, Train loss: 764.0611, Val loss: 1387.9698, Best Val loss: 1387.9083\n",
            "Epoch 72/100, Train loss: 831.9756, Val loss: 1388.0298, Best Val loss: 1387.9083\n",
            "Epoch 73/100, Train loss: 780.4026, Val loss: 1388.0085, Best Val loss: 1387.9083\n",
            "Epoch 74/100, Train loss: 822.3936, Val loss: 1387.9861, Best Val loss: 1387.9083\n",
            "Epoch 75/100, Train loss: 809.6502, Val loss: 1387.8581, Best Val loss: 1387.8581\n",
            "Epoch 76/100, Train loss: 804.0861, Val loss: 1387.7785, Best Val loss: 1387.7785\n",
            "Epoch 77/100, Train loss: 803.5931, Val loss: 1387.7538, Best Val loss: 1387.7538\n",
            "Epoch 78/100, Train loss: 771.8626, Val loss: 1387.7101, Best Val loss: 1387.7101\n",
            "Epoch 79/100, Train loss: 767.9741, Val loss: 1387.6720, Best Val loss: 1387.6720\n",
            "Epoch 80/100, Train loss: 783.5249, Val loss: 1387.7365, Best Val loss: 1387.6720\n",
            "Epoch 81/100, Train loss: 796.1022, Val loss: 1387.6742, Best Val loss: 1387.6720\n",
            "Epoch 82/100, Train loss: 764.6440, Val loss: 1387.6432, Best Val loss: 1387.6432\n",
            "Epoch 83/100, Train loss: 764.4420, Val loss: 1387.6638, Best Val loss: 1387.6432\n",
            "Epoch 84/100, Train loss: 771.3698, Val loss: 1387.6742, Best Val loss: 1387.6432\n",
            "Epoch 85/100, Train loss: 757.9113, Val loss: 1387.6861, Best Val loss: 1387.6432\n",
            "Epoch 86/100, Train loss: 798.1074, Val loss: 1387.6898, Best Val loss: 1387.6432\n",
            "Epoch 87/100, Train loss: 802.2145, Val loss: 1387.6909, Best Val loss: 1387.6432\n",
            "Epoch 88/100, Train loss: 760.8248, Val loss: 1387.6802, Best Val loss: 1387.6432\n",
            "Epoch 89/100, Train loss: 761.9670, Val loss: 1387.6740, Best Val loss: 1387.6432\n",
            "Epoch 90/100, Train loss: 796.8971, Val loss: 1387.6899, Best Val loss: 1387.6432\n",
            "Epoch 91/100, Train loss: 769.2217, Val loss: 1387.6882, Best Val loss: 1387.6432\n",
            "Epoch 92/100, Train loss: 775.6884, Val loss: 1387.6796, Best Val loss: 1387.6432\n",
            "Epoch 93/100, Train loss: 758.1346, Val loss: 1387.6724, Best Val loss: 1387.6432\n",
            "Epoch 94/100, Train loss: 794.5696, Val loss: 1387.6728, Best Val loss: 1387.6432\n",
            "Epoch 95/100, Train loss: 758.1413, Val loss: 1387.6733, Best Val loss: 1387.6432\n",
            "Epoch 96/100, Train loss: 756.9498, Val loss: 1387.6763, Best Val loss: 1387.6432\n",
            "Epoch 97/100, Train loss: 796.4798, Val loss: 1387.6765, Best Val loss: 1387.6432\n",
            "Epoch 98/100, Train loss: 756.1021, Val loss: 1387.6761, Best Val loss: 1387.6432\n",
            "Epoch 99/100, Train loss: 761.0909, Val loss: 1387.6763, Best Val loss: 1387.6432\n",
            "Epoch 100/100, Train loss: 782.6382, Val loss: 1387.6763, Best Val loss: 1387.6432\n",
            "Epoch 1/100, Train loss: 1122.2835, Val loss: 1910.2540, Best Val loss: 1910.2540\n",
            "Epoch 2/100, Train loss: 1125.8285, Val loss: 1860.5048, Best Val loss: 1860.5048\n",
            "Epoch 3/100, Train loss: 1126.1720, Val loss: 1814.3494, Best Val loss: 1814.3494\n",
            "Epoch 4/100, Train loss: 1037.0092, Val loss: 1771.2788, Best Val loss: 1771.2788\n",
            "Epoch 5/100, Train loss: 1052.0373, Val loss: 1733.3519, Best Val loss: 1733.3519\n",
            "Epoch 6/100, Train loss: 975.6022, Val loss: 1697.9515, Best Val loss: 1697.9515\n",
            "Epoch 7/100, Train loss: 962.9818, Val loss: 1666.7672, Best Val loss: 1666.7672\n",
            "Epoch 8/100, Train loss: 873.7755, Val loss: 1637.6381, Best Val loss: 1637.6381\n",
            "Epoch 9/100, Train loss: 875.9825, Val loss: 1613.4797, Best Val loss: 1613.4797\n",
            "Epoch 10/100, Train loss: 834.8742, Val loss: 1590.6755, Best Val loss: 1590.6755\n",
            "Epoch 11/100, Train loss: 825.9562, Val loss: 1570.1690, Best Val loss: 1570.1690\n",
            "Epoch 12/100, Train loss: 854.2922, Val loss: 1553.0897, Best Val loss: 1553.0897\n",
            "Epoch 13/100, Train loss: 809.4752, Val loss: 1536.4214, Best Val loss: 1536.4214\n",
            "Epoch 14/100, Train loss: 799.7037, Val loss: 1522.6403, Best Val loss: 1522.6403\n",
            "Epoch 15/100, Train loss: 817.5357, Val loss: 1509.9121, Best Val loss: 1509.9121\n",
            "Epoch 16/100, Train loss: 776.9207, Val loss: 1499.0087, Best Val loss: 1499.0087\n",
            "Epoch 17/100, Train loss: 761.6640, Val loss: 1489.1186, Best Val loss: 1489.1186\n",
            "Epoch 18/100, Train loss: 847.8497, Val loss: 1481.9220, Best Val loss: 1481.9220\n",
            "Epoch 19/100, Train loss: 754.4906, Val loss: 1473.8114, Best Val loss: 1473.8114\n",
            "Epoch 20/100, Train loss: 751.8822, Val loss: 1467.4993, Best Val loss: 1467.4993\n",
            "Epoch 21/100, Train loss: 793.4082, Val loss: 1461.3536, Best Val loss: 1461.3536\n",
            "Epoch 22/100, Train loss: 747.9701, Val loss: 1455.9026, Best Val loss: 1455.9026\n",
            "Epoch 23/100, Train loss: 757.0553, Val loss: 1451.5929, Best Val loss: 1451.5929\n",
            "Epoch 24/100, Train loss: 743.3453, Val loss: 1447.7612, Best Val loss: 1447.7612\n",
            "Epoch 25/100, Train loss: 753.9997, Val loss: 1444.8539, Best Val loss: 1444.8539\n",
            "Epoch 26/100, Train loss: 771.4758, Val loss: 1441.5861, Best Val loss: 1441.5861\n",
            "Epoch 27/100, Train loss: 785.4499, Val loss: 1438.9675, Best Val loss: 1438.9675\n",
            "Epoch 28/100, Train loss: 744.5978, Val loss: 1436.7793, Best Val loss: 1436.7793\n",
            "Epoch 29/100, Train loss: 800.7165, Val loss: 1434.6210, Best Val loss: 1434.6210\n",
            "Epoch 30/100, Train loss: 741.2132, Val loss: 1432.8208, Best Val loss: 1432.8208\n",
            "Epoch 31/100, Train loss: 732.4752, Val loss: 1431.4161, Best Val loss: 1431.4161\n",
            "Epoch 32/100, Train loss: 778.8581, Val loss: 1430.0307, Best Val loss: 1430.0307\n",
            "Epoch 33/100, Train loss: 729.5888, Val loss: 1428.4533, Best Val loss: 1428.4533\n",
            "Epoch 34/100, Train loss: 741.1243, Val loss: 1427.9254, Best Val loss: 1427.9254\n",
            "Epoch 35/100, Train loss: 784.9786, Val loss: 1426.8647, Best Val loss: 1426.8647\n",
            "Epoch 36/100, Train loss: 730.6749, Val loss: 1426.1183, Best Val loss: 1426.1183\n",
            "Epoch 37/100, Train loss: 728.7324, Val loss: 1425.4945, Best Val loss: 1425.4945\n",
            "Epoch 38/100, Train loss: 765.6503, Val loss: 1424.9350, Best Val loss: 1424.9350\n",
            "Epoch 39/100, Train loss: 782.7946, Val loss: 1424.5816, Best Val loss: 1424.5816\n",
            "Epoch 40/100, Train loss: 731.2987, Val loss: 1424.0530, Best Val loss: 1424.0530\n",
            "Epoch 41/100, Train loss: 759.2782, Val loss: 1423.6234, Best Val loss: 1423.6234\n",
            "Epoch 42/100, Train loss: 767.0076, Val loss: 1423.2713, Best Val loss: 1423.2713\n",
            "Epoch 43/100, Train loss: 752.8040, Val loss: 1422.9166, Best Val loss: 1422.9166\n",
            "Epoch 44/100, Train loss: 736.0979, Val loss: 1422.5599, Best Val loss: 1422.5599\n",
            "Epoch 45/100, Train loss: 794.4235, Val loss: 1422.2884, Best Val loss: 1422.2884\n",
            "Epoch 46/100, Train loss: 728.0804, Val loss: 1421.8350, Best Val loss: 1421.8350\n",
            "Epoch 47/100, Train loss: 738.0281, Val loss: 1421.6515, Best Val loss: 1421.6515\n",
            "Epoch 48/100, Train loss: 726.7157, Val loss: 1421.4472, Best Val loss: 1421.4472\n",
            "Epoch 49/100, Train loss: 730.6265, Val loss: 1421.5538, Best Val loss: 1421.4472\n",
            "Epoch 50/100, Train loss: 727.6956, Val loss: 1421.2478, Best Val loss: 1421.2478\n",
            "Epoch 51/100, Train loss: 798.3858, Val loss: 1421.2585, Best Val loss: 1421.2478\n",
            "Epoch 52/100, Train loss: 727.8235, Val loss: 1421.2215, Best Val loss: 1421.2215\n",
            "Epoch 53/100, Train loss: 772.1688, Val loss: 1421.0720, Best Val loss: 1421.0720\n",
            "Epoch 54/100, Train loss: 739.5566, Val loss: 1420.8447, Best Val loss: 1420.8447\n",
            "Epoch 55/100, Train loss: 769.1689, Val loss: 1421.1346, Best Val loss: 1420.8447\n",
            "Epoch 56/100, Train loss: 772.2519, Val loss: 1420.7924, Best Val loss: 1420.7924\n",
            "Epoch 57/100, Train loss: 766.1573, Val loss: 1420.7627, Best Val loss: 1420.7627\n",
            "Epoch 58/100, Train loss: 731.6020, Val loss: 1420.5007, Best Val loss: 1420.5007\n",
            "Epoch 59/100, Train loss: 784.9864, Val loss: 1420.7639, Best Val loss: 1420.5007\n",
            "Epoch 60/100, Train loss: 726.1646, Val loss: 1420.3432, Best Val loss: 1420.3432\n",
            "Epoch 61/100, Train loss: 726.7711, Val loss: 1420.1355, Best Val loss: 1420.1355\n",
            "Epoch 62/100, Train loss: 740.1887, Val loss: 1420.2065, Best Val loss: 1420.1355\n",
            "Epoch 63/100, Train loss: 727.4156, Val loss: 1420.2760, Best Val loss: 1420.1355\n",
            "Epoch 64/100, Train loss: 730.5522, Val loss: 1420.2884, Best Val loss: 1420.1355\n",
            "Epoch 65/100, Train loss: 808.7632, Val loss: 1420.3285, Best Val loss: 1420.1355\n",
            "Epoch 66/100, Train loss: 738.9815, Val loss: 1420.2843, Best Val loss: 1420.1355\n",
            "Epoch 67/100, Train loss: 742.8456, Val loss: 1420.2196, Best Val loss: 1420.1355\n",
            "Epoch 68/100, Train loss: 775.7510, Val loss: 1420.0952, Best Val loss: 1420.0952\n",
            "Epoch 69/100, Train loss: 730.2124, Val loss: 1420.0084, Best Val loss: 1420.0084\n",
            "Epoch 70/100, Train loss: 730.2198, Val loss: 1420.0251, Best Val loss: 1420.0084\n",
            "Epoch 71/100, Train loss: 733.5832, Val loss: 1420.0849, Best Val loss: 1420.0084\n",
            "Epoch 72/100, Train loss: 801.6522, Val loss: 1420.1442, Best Val loss: 1420.0084\n",
            "Epoch 73/100, Train loss: 749.2630, Val loss: 1420.1189, Best Val loss: 1420.0084\n",
            "Epoch 74/100, Train loss: 793.4112, Val loss: 1420.1024, Best Val loss: 1420.0084\n",
            "Epoch 75/100, Train loss: 776.2797, Val loss: 1419.9681, Best Val loss: 1419.9681\n",
            "Epoch 76/100, Train loss: 772.8603, Val loss: 1419.8831, Best Val loss: 1419.8831\n",
            "Epoch 77/100, Train loss: 771.9168, Val loss: 1419.8570, Best Val loss: 1419.8570\n",
            "Epoch 78/100, Train loss: 740.9970, Val loss: 1419.8125, Best Val loss: 1419.8125\n",
            "Epoch 79/100, Train loss: 737.5463, Val loss: 1419.7727, Best Val loss: 1419.7727\n",
            "Epoch 80/100, Train loss: 751.2672, Val loss: 1419.8417, Best Val loss: 1419.7727\n",
            "Epoch 81/100, Train loss: 765.9578, Val loss: 1419.7793, Best Val loss: 1419.7727\n",
            "Epoch 82/100, Train loss: 730.3693, Val loss: 1419.7460, Best Val loss: 1419.7460\n",
            "Epoch 83/100, Train loss: 733.5771, Val loss: 1419.7681, Best Val loss: 1419.7460\n",
            "Epoch 84/100, Train loss: 740.1343, Val loss: 1419.7811, Best Val loss: 1419.7460\n",
            "Epoch 85/100, Train loss: 727.7964, Val loss: 1419.7923, Best Val loss: 1419.7460\n",
            "Epoch 86/100, Train loss: 764.8297, Val loss: 1419.7972, Best Val loss: 1419.7460\n",
            "Epoch 87/100, Train loss: 768.3289, Val loss: 1419.7978, Best Val loss: 1419.7460\n",
            "Epoch 88/100, Train loss: 730.8153, Val loss: 1419.7879, Best Val loss: 1419.7460\n",
            "Epoch 89/100, Train loss: 731.4664, Val loss: 1419.7796, Best Val loss: 1419.7460\n",
            "Epoch 90/100, Train loss: 764.4612, Val loss: 1419.7973, Best Val loss: 1419.7460\n",
            "Epoch 91/100, Train loss: 737.7009, Val loss: 1419.7956, Best Val loss: 1419.7460\n",
            "Epoch 92/100, Train loss: 743.7654, Val loss: 1419.7879, Best Val loss: 1419.7460\n",
            "Epoch 93/100, Train loss: 727.7553, Val loss: 1419.7797, Best Val loss: 1419.7460\n",
            "Epoch 94/100, Train loss: 763.0537, Val loss: 1419.7803, Best Val loss: 1419.7460\n",
            "Epoch 95/100, Train loss: 728.0153, Val loss: 1419.7805, Best Val loss: 1419.7460\n",
            "Epoch 96/100, Train loss: 726.8278, Val loss: 1419.7839, Best Val loss: 1419.7460\n",
            "Epoch 97/100, Train loss: 763.7640, Val loss: 1419.7843, Best Val loss: 1419.7460\n",
            "Epoch 98/100, Train loss: 726.0363, Val loss: 1419.7839, Best Val loss: 1419.7460\n",
            "Epoch 99/100, Train loss: 731.0839, Val loss: 1419.7839, Best Val loss: 1419.7460\n",
            "Epoch 100/100, Train loss: 748.6227, Val loss: 1419.7840, Best Val loss: 1419.7460\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Window Size  Step Size  MSE (Raw Input)\n",
              "0           10          5      1105.132812\n",
              "1           10         10      1378.377197\n",
              "2            5         10      1365.297241"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-58655b85-af24-4a27-a0e5-52af053b7e7e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Window Size</th>\n",
              "      <th>Step Size</th>\n",
              "      <th>MSE (Raw Input)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>1105.132812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>1378.377197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>1365.297241</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58655b85-af24-4a27-a0e5-52af053b7e7e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-58655b85-af24-4a27-a0e5-52af053b7e7e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-58655b85-af24-4a27-a0e5-52af053b7e7e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-51263664-7b1d-4de8-b687-ddcd1dd0314f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-51263664-7b1d-4de8-b687-ddcd1dd0314f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-51263664-7b1d-4de8-b687-ddcd1dd0314f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Window Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 5,\n        \"max\": 10,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          5,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Step Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 5,\n        \"max\": 10,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          10,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MSE (Raw Input)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1105.1328125,\n          1378.377197265625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZgnW2u_hIrr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Normalized input**"
      ],
      "metadata": {
        "id": "0yzteNgXIs2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment_normalize(df, window_size, step, with_volume=False, seed=42):\n",
        "    # 🔒 Set random seed\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # 🔺 Feature selection\n",
        "    if with_volume:\n",
        "        features = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "        input_dim = 5\n",
        "    else:\n",
        "        features = df[['Open', 'High', 'Low', 'Close']]\n",
        "        input_dim = 4\n",
        "\n",
        "    labels = df['High'].shift(-1).dropna()\n",
        "    features = features.iloc[:-1]\n",
        "\n",
        "    # Create sequences\n",
        "    X, y = create_sequences(features.values, labels.values, window_size, step)\n",
        "\n",
        "    # Test set split\n",
        "    ind = np.linspace(0, len(X) - 1, num=int(len(X) * 0.1), dtype=int)\n",
        "    x_test_raw, y_test = X[ind], y[ind]\n",
        "    X, y = np.delete(X, ind, axis=0), np.delete(y, ind, axis=0)\n",
        "\n",
        "    # Shuffle & train/val split\n",
        "    idx = np.random.permutation(len(X))\n",
        "    X, y = X[idx], y[idx]\n",
        "    split = int(len(X) * 0.8)\n",
        "    x_train_raw, y_train = X[:split], y[:split]\n",
        "    x_val_raw, y_val = X[split:], y[split:]\n",
        "\n",
        "    # Scaling\n",
        "    scaler_x = MinMaxScaler().fit(x_train_raw.reshape(-1, x_train_raw.shape[-1]))\n",
        "    x_train = scaler_x.transform(x_train_raw.reshape(-1, x_train_raw.shape[-1])).reshape(x_train_raw.shape)\n",
        "    x_val = scaler_x.transform(x_val_raw.reshape(-1, x_val_raw.shape[-1])).reshape(x_val_raw.shape)\n",
        "    x_test = scaler_x.transform(x_test_raw.reshape(-1, x_test_raw.shape[-1])).reshape(x_test_raw.shape)\n",
        "\n",
        "    scaler_y = MinMaxScaler().fit(y_train.reshape(-1, 1))\n",
        "    y_train = scaler_y.transform(y_train.reshape(-1, 1)).reshape(-1)\n",
        "    y_val = scaler_y.transform(y_val.reshape(-1, 1)).reshape(-1)\n",
        "    y_test = scaler_y.transform(y_test.reshape(-1, 1)).reshape(-1)\n",
        "\n",
        "    # Tensors\n",
        "    x_train = torch.from_numpy(x_train).float()\n",
        "    y_train = torch.from_numpy(y_train).float()\n",
        "    x_val = torch.from_numpy(x_val).float()\n",
        "    y_val = torch.from_numpy(y_val).float()\n",
        "    x_test = torch.from_numpy(x_test).float()\n",
        "    y_test = torch.from_numpy(y_test).float()\n",
        "\n",
        "    train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(x_val, y_val), batch_size=32)\n",
        "    test_loader = DataLoader(TensorDataset(x_test, y_test), batch_size=32)\n",
        "\n",
        "    # Model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = LSTMModel(input_dim=input_dim, hidden_dim=500, num_layers=1, output_dim=1).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    lr_scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for features, labels in train_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            outputs = model(features).squeeze(-1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        lr_scheduler.step()\n",
        "        train_losses.append(total_loss / len(train_loader))\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for features, labels in val_loader:\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "                outputs = model(features).squeeze(-1)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f'Epoch {epoch+1:03} | Train Loss: {train_losses[-1]:.4f} | Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "    # Test\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    model.eval()\n",
        "    preds, actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for features, labels in test_loader:\n",
        "            features = features.to(device)\n",
        "            pred = model(features).squeeze(-1).cpu()\n",
        "            preds.append(pred)\n",
        "            actuals.append(labels)\n",
        "    pred_value = torch.cat(preds)\n",
        "    actual_value = torch.cat(actuals)\n",
        "\n",
        "    pred_real = scaler_y.inverse_transform(pred_value.numpy().reshape(-1, 1)).reshape(-1)\n",
        "    actual_real = scaler_y.inverse_transform(actual_value.numpy().reshape(-1, 1)).reshape(-1)\n",
        "    test_mse = np.mean((pred_real - actual_real) ** 2)\n",
        "    return round(test_mse, 6)\n"
      ],
      "metadata": {
        "id": "0_61a2dGgrpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = [\n",
        "    (10, 5),   # window > step\n",
        "    (10, 10),  # window = step\n",
        "    (5, 10)    # window < step\n",
        "]\n",
        "results = []\n",
        "\n",
        "for w, s in params:\n",
        "    mse = run_experiment_normalize(df, window_size=w, step=s, with_volume=False)\n",
        "    results.append({'Window Size': w, 'Step Size': s, 'MSE (Normalize Input)': mse})\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YxIVDFOCKMzR",
        "outputId": "83d2a66f-9733-48d5-d9e9-59b37acf417d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Train Loss: 0.0132 | Val Loss: 0.0081\n",
            "Epoch 002 | Train Loss: 0.0087 | Val Loss: 0.0013\n",
            "Epoch 003 | Train Loss: 0.0018 | Val Loss: 0.0011\n",
            "Epoch 004 | Train Loss: 0.0005 | Val Loss: 0.0002\n",
            "Epoch 005 | Train Loss: 0.0002 | Val Loss: 0.0001\n",
            "Epoch 006 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 007 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 008 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 009 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 010 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 011 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 012 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 013 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 014 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 015 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 016 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 017 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 018 | Train Loss: 0.0001 | Val Loss: 0.0002\n",
            "Epoch 019 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 020 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 021 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 022 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 023 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 024 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 025 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 026 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 027 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 028 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 029 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 030 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 031 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 032 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 033 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 034 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 035 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 036 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 037 | Train Loss: 0.0001 | Val Loss: 0.0002\n",
            "Epoch 038 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 039 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 040 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 041 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 042 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 043 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 044 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 045 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 046 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 047 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 048 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 049 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 050 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 051 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 052 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 053 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 054 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 055 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 056 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 057 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 058 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 059 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 060 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 061 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 062 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 063 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 064 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 065 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 066 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 067 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 068 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 069 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 070 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 071 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 072 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 073 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 074 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 075 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 076 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 077 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 078 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 079 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 080 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 081 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 082 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 083 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 084 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 085 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 086 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 087 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 088 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 089 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 090 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 091 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 092 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 093 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 094 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 095 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 096 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 097 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 098 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 099 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 100 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 001 | Train Loss: 0.0116 | Val Loss: 0.0034\n",
            "Epoch 002 | Train Loss: 0.0027 | Val Loss: 0.0014\n",
            "Epoch 003 | Train Loss: 0.0007 | Val Loss: 0.0016\n",
            "Epoch 004 | Train Loss: 0.0004 | Val Loss: 0.0003\n",
            "Epoch 005 | Train Loss: 0.0002 | Val Loss: 0.0001\n",
            "Epoch 006 | Train Loss: 0.0001 | Val Loss: 0.0004\n",
            "Epoch 007 | Train Loss: 0.0002 | Val Loss: 0.0001\n",
            "Epoch 008 | Train Loss: 0.0001 | Val Loss: 0.0003\n",
            "Epoch 009 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 010 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 011 | Train Loss: 0.0001 | Val Loss: 0.0002\n",
            "Epoch 012 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 013 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 014 | Train Loss: 0.0001 | Val Loss: 0.0002\n",
            "Epoch 015 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 016 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 017 | Train Loss: 0.0001 | Val Loss: 0.0002\n",
            "Epoch 018 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 019 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 020 | Train Loss: 0.0001 | Val Loss: 0.0003\n",
            "Epoch 021 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 022 | Train Loss: 0.0001 | Val Loss: 0.0002\n",
            "Epoch 023 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 024 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 025 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 026 | Train Loss: 0.0001 | Val Loss: 0.0002\n",
            "Epoch 027 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 028 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 029 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 030 | Train Loss: 0.0001 | Val Loss: 0.0002\n",
            "Epoch 031 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 032 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 033 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 034 | Train Loss: 0.0001 | Val Loss: 0.0002\n",
            "Epoch 035 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 036 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 037 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 038 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 039 | Train Loss: 0.0001 | Val Loss: 0.0002\n",
            "Epoch 040 | Train Loss: 0.0001 | Val Loss: 0.0002\n",
            "Epoch 041 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 042 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 043 | Train Loss: 0.0001 | Val Loss: 0.0003\n",
            "Epoch 044 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 045 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 046 | Train Loss: 0.0001 | Val Loss: 0.0003\n",
            "Epoch 047 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 048 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 049 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 050 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 051 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 052 | Train Loss: 0.0001 | Val Loss: 0.0003\n",
            "Epoch 053 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 054 | Train Loss: 0.0001 | Val Loss: 0.0002\n",
            "Epoch 055 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 056 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 057 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 058 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 059 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 060 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 061 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 062 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 063 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 064 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 065 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 066 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 067 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 068 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 069 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 070 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 071 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 072 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 073 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 074 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 075 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 076 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 077 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 078 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 079 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 080 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 081 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 082 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 083 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 084 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 085 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 086 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 087 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 088 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 089 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 090 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 091 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 092 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 093 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 094 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 095 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 096 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 097 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 098 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 099 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 100 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 001 | Train Loss: 0.0129 | Val Loss: 0.0005\n",
            "Epoch 002 | Train Loss: 0.0029 | Val Loss: 0.0046\n",
            "Epoch 003 | Train Loss: 0.0030 | Val Loss: 0.0009\n",
            "Epoch 004 | Train Loss: 0.0008 | Val Loss: 0.0002\n",
            "Epoch 005 | Train Loss: 0.0006 | Val Loss: 0.0002\n",
            "Epoch 006 | Train Loss: 0.0002 | Val Loss: 0.0006\n",
            "Epoch 007 | Train Loss: 0.0003 | Val Loss: 0.0000\n",
            "Epoch 008 | Train Loss: 0.0002 | Val Loss: 0.0001\n",
            "Epoch 009 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 010 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 011 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 012 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 013 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 014 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 015 | Train Loss: 0.0002 | Val Loss: 0.0001\n",
            "Epoch 016 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 017 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 018 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 019 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 020 | Train Loss: 0.0002 | Val Loss: 0.0001\n",
            "Epoch 021 | Train Loss: 0.0002 | Val Loss: 0.0000\n",
            "Epoch 022 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 023 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 024 | Train Loss: 0.0002 | Val Loss: 0.0001\n",
            "Epoch 025 | Train Loss: 0.0002 | Val Loss: 0.0001\n",
            "Epoch 026 | Train Loss: 0.0002 | Val Loss: 0.0001\n",
            "Epoch 027 | Train Loss: 0.0002 | Val Loss: 0.0004\n",
            "Epoch 028 | Train Loss: 0.0003 | Val Loss: 0.0002\n",
            "Epoch 029 | Train Loss: 0.0002 | Val Loss: 0.0002\n",
            "Epoch 030 | Train Loss: 0.0002 | Val Loss: 0.0003\n",
            "Epoch 031 | Train Loss: 0.0002 | Val Loss: 0.0001\n",
            "Epoch 032 | Train Loss: 0.0001 | Val Loss: 0.0002\n",
            "Epoch 033 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 034 | Train Loss: 0.0002 | Val Loss: 0.0002\n",
            "Epoch 035 | Train Loss: 0.0001 | Val Loss: 0.0005\n",
            "Epoch 036 | Train Loss: 0.0002 | Val Loss: 0.0002\n",
            "Epoch 037 | Train Loss: 0.0002 | Val Loss: 0.0001\n",
            "Epoch 038 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 039 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 040 | Train Loss: 0.0002 | Val Loss: 0.0000\n",
            "Epoch 041 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 042 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 043 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 044 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 045 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 046 | Train Loss: 0.0002 | Val Loss: 0.0001\n",
            "Epoch 047 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 048 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 049 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 050 | Train Loss: 0.0002 | Val Loss: 0.0000\n",
            "Epoch 051 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 052 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 053 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 054 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 055 | Train Loss: 0.0002 | Val Loss: 0.0000\n",
            "Epoch 056 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 057 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 058 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 059 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 060 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 061 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 062 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 063 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 064 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 065 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 066 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
            "Epoch 067 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 068 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 069 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 070 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 071 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 072 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 073 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 074 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 075 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 076 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 077 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 078 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 079 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 080 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 081 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 082 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 083 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 084 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 085 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 086 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 087 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 088 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 089 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 090 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 091 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 092 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 093 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 094 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 095 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 096 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 097 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 098 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 099 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
            "Epoch 100 | Train Loss: 0.0001 | Val Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Window Size  Step Size  MSE (Normalize Input)\n",
              "0           10          5               0.760277\n",
              "1           10         10               1.420198\n",
              "2            5         10               2.604735"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6ed3b3b1-05cf-40e1-8f42-ef32ce3cf95d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Window Size</th>\n",
              "      <th>Step Size</th>\n",
              "      <th>MSE (Normalize Input)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>0.760277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>1.420198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>2.604735</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ed3b3b1-05cf-40e1-8f42-ef32ce3cf95d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6ed3b3b1-05cf-40e1-8f42-ef32ce3cf95d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6ed3b3b1-05cf-40e1-8f42-ef32ce3cf95d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-059b3c97-0684-4153-88ea-7a9e2165ee31\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-059b3c97-0684-4153-88ea-7a9e2165ee31')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-059b3c97-0684-4153-88ea-7a9e2165ee31 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Window Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 5,\n        \"max\": 10,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          5,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Step Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 5,\n        \"max\": 10,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          10,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MSE (Normalize Input)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.7602769732475281,\n          1.4201979637145996\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SUyGk4Uu0Yq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Different Feature Combinations**"
      ],
      "metadata": {
        "id": "Gj8N_aqmlQhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment_normalize_custom(df, window_size, step, feature_cols, seed=42):\n",
        "    # Set random seed\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Feature selection\n",
        "    features = df[feature_cols].copy()\n",
        "    input_dim = features.shape[1]\n",
        "    labels = df['High'].shift(-1).dropna()\n",
        "    features = features.iloc[:-1]\n",
        "\n",
        "    # Sequence\n",
        "    X, y = create_sequences(features.values, labels.values, window_size, step)\n",
        "    ind = np.linspace(0, len(X) - 1, num=int(len(X) * 0.1), dtype=int)\n",
        "    x_test_raw, y_test = X[ind], y[ind]\n",
        "    X, y = np.delete(X, ind, axis=0), np.delete(y, ind, axis=0)\n",
        "\n",
        "    idx = np.random.permutation(len(X))\n",
        "    X, y = X[idx], y[idx]\n",
        "    split = int(len(X) * 0.8)\n",
        "    x_train_raw, y_train = X[:split], y[:split]\n",
        "    x_val_raw, y_val = X[split:], y[split:]\n",
        "\n",
        "    scaler_x = MinMaxScaler().fit(x_train_raw.reshape(-1, input_dim))\n",
        "    x_train = scaler_x.transform(x_train_raw.reshape(-1, input_dim)).reshape(x_train_raw.shape)\n",
        "    x_val = scaler_x.transform(x_val_raw.reshape(-1, input_dim)).reshape(x_val_raw.shape)\n",
        "    x_test = scaler_x.transform(x_test_raw.reshape(-1, input_dim)).reshape(x_test_raw.shape)\n",
        "\n",
        "    scaler_y = MinMaxScaler().fit(y_train.reshape(-1, 1))\n",
        "    y_train = scaler_y.transform(y_train.reshape(-1, 1)).reshape(-1)\n",
        "    y_val = scaler_y.transform(y_val.reshape(-1, 1)).reshape(-1)\n",
        "    y_test = scaler_y.transform(y_test.reshape(-1, 1)).reshape(-1)\n",
        "\n",
        "    # Tensor\n",
        "    x_train = torch.from_numpy(x_train).float()\n",
        "    y_train = torch.from_numpy(y_train).float()\n",
        "    x_val = torch.from_numpy(x_val).float()\n",
        "    y_val = torch.from_numpy(y_val).float()\n",
        "    x_test = torch.from_numpy(x_test).float()\n",
        "    y_test = torch.from_numpy(y_test).float()\n",
        "\n",
        "    train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(x_val, y_val), batch_size=32)\n",
        "    test_loader = DataLoader(TensorDataset(x_test, y_test), batch_size=32)\n",
        "\n",
        "    # Model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = LSTMModel(input_dim=input_dim, hidden_dim=500, num_layers=1, output_dim=1).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    lr_scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for features, labels in train_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            outputs = model(features).squeeze(-1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for features, labels in val_loader:\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "                outputs = model(features).squeeze(-1)\n",
        "                val_loss += loss.item()\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    # Test\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    model.eval()\n",
        "    preds, actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for features, labels in test_loader:\n",
        "            features = features.to(device)\n",
        "            pred = model(features).squeeze(-1).cpu()\n",
        "            preds.append(pred)\n",
        "            actuals.append(labels)\n",
        "    pred_value = torch.cat(preds).numpy()\n",
        "    actual_value = torch.cat(actuals).numpy()\n",
        "\n",
        "    pred_real = scaler_y.inverse_transform(pred_value.reshape(-1, 1)).reshape(-1)\n",
        "    actual_real = scaler_y.inverse_transform(actual_value.reshape(-1, 1)).reshape(-1)\n",
        "    test_mse = np.mean((pred_real - actual_real) ** 2)\n",
        "    return round(test_mse, 6)\n"
      ],
      "metadata": {
        "id": "63Q0e7PlEuEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_sets = {\n",
        "    \"Close\": ['Close'],\n",
        "    \"Open, Close\": ['Open', 'Close'],\n",
        "    \"Close, Volume\": ['Close', 'Volume'],\n",
        "    \"High, Low, Close\": ['High', 'Low', 'Close'],\n",
        "    \"Open, High, Low, Close\": ['Open', 'High', 'Low', 'Close'],\n",
        "    \"Open, High, Low, Close, Volume\": ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, feature_cols in feature_sets.items():\n",
        "    mse = run_experiment_normalize_custom(df, window_size=10, step=5, feature_cols=feature_cols)\n",
        "    results.append({'Feature Set': name, 'MSE': mse})\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "jYR9pUg02xru",
        "outputId": "d1a357ee-298f-48db-fdaf-a554db376947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      Feature Set       MSE\n",
              "0                           Close  0.996654\n",
              "1                     Open, Close  0.775718\n",
              "2                   Close, Volume  0.840096\n",
              "3                High, Low, Close  1.385323\n",
              "4          Open, High, Low, Close  1.124557\n",
              "5  Open, High, Low, Close, Volume  0.804809"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-42963880-f209-46a6-9021-2ceb3e4e94d8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature Set</th>\n",
              "      <th>MSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Close</td>\n",
              "      <td>0.996654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Open, Close</td>\n",
              "      <td>0.775718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Close, Volume</td>\n",
              "      <td>0.840096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>High, Low, Close</td>\n",
              "      <td>1.385323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Open, High, Low, Close</td>\n",
              "      <td>1.124557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Open, High, Low, Close, Volume</td>\n",
              "      <td>0.804809</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42963880-f209-46a6-9021-2ceb3e4e94d8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-42963880-f209-46a6-9021-2ceb3e4e94d8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-42963880-f209-46a6-9021-2ceb3e4e94d8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3e8f3916-b6c2-4fed-bb7c-17b4c41416ff\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3e8f3916-b6c2-4fed-bb7c-17b4c41416ff')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3e8f3916-b6c2-4fed-bb7c-17b4c41416ff button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"Feature Set\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Close\",\n          \"Open, Close\",\n          \"Open, High, Low, Close, Volume\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MSE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.9966539740562439,\n          0.7757179737091064,\n          0.8048089742660522\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "itSl0n3C2xtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FnBla-yi2xv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7yjZNNP_2pKC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}